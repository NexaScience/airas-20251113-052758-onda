
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/` directory, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "DNN-MFBO hard-wires the previous-fidelity output fm-1(x) as an unscaled input feature for fm(x).\nIf the correlation between fidelities is weak or changes locally, this unconditional use of fm-1(x) can introduce negative transfer, slowing hyper-parameter search and occasionally selecting misleading low-cost queries. A minimal mechanism is needed to adaptively reduce or amplify the influence of lower fidelities without redesigning the architecture or acquisition.",
    "Methods": "Reliability-Aware DNN-MFBO (RA-DNN-MFBO)\nMinimal change: insert one learnable scalar gate αm∈[0,1] for every fidelity m>1.\nInput becomes   xm=[x ; αm·fm-1(x)]   instead of   [x ; fm-1(x)].\nPrior: αm∼Beta(1,1)  (uniform). In variational inference we keep a Gaussian variational posterior on logit(αm). An extra KL( q(αm) || p(αm) ) term is added to the ELBO (two additional lines of code).\nMotivation:\n• If fm-1 is unreliable, the posterior pushes αm→0, avoiding negative transfer.\n• When fidelities are strongly correlated the network learns αm→1, recovering the original model.\n• This single multiplicative gate is differentiable, cheap, and does not change acquisition-function derivations (moment matching still applies because scaling a Gaussian keeps it Gaussian).",
    "Experimental Setup": "Data: 2-fidelity Branin and Levy benchmarks supplied with DNN-MFBO code (low-fidelity = cheap interpolation, high-fidelity = ground truth).\nBaselines: Original DNN-MFBO vs proposed RA-DNN-MFBO.\nInitial design: 10 random queries per fidelity.\nBudget: cost-weighted budget of 150 (low-fidelity cost 1, high-fidelity cost 10).\nMetrics: simple regret w.r.t. cost, plus nRMSE of surrogate on 100 test points to isolate modelling effect.\nRepeats: 10 random seeds.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass FidelityBlock(nn.Module):\n    def __init__(self, in_dim, hid=50):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(in_dim, hid), nn.ReLU(),\n                                 nn.Linear(hid, 1))\n        self.log_var = nn.Parameter(torch.zeros(1))  # homoscedastic noise\n    def forward(self, x):\n        mean = self.net(x)\n        var  = torch.exp(self.log_var)\n        return mean, var\n\nclass RADNNMFBO(nn.Module):\n    \"\"\"2-fidelity version for demo\"\"\"\n    def __init__(self, x_dim):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim)\n        self.alpha_raw = nn.Parameter(torch.zeros(1))  # sigmoid→α∈(0,1)\n        self.f2 = FidelityBlock(x_dim+1)\n    def forward_f1(self, x):\n        return self.f1(x)\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1,_ = self.f1(x)\n        alpha = torch.sigmoid(self.alpha_raw)\n        inp = torch.cat([x, alpha*mean1], dim=-1)\n        return self.f2(inp)\n# --- tiny training loop for surrogate only ---\nopt = torch.optim.Adam(model.parameters(), 1e-3)\nfor itr in range(1000):\n    m1,v1 = model.forward_f1(x1);  m2,v2 = model.forward_f2(x2)\n    nll  = ((y1-m1)**2/v1 + torch.log(v1)).mean()\n    nll += ((y2-m2)**2/v2 + torch.log(v2)).mean()\n    # Beta(1,1) prior KL on α (Gaussian approx):\n    kl_alpha = -torch.distributions.Beta(1,1).log_prob(torch.sigmoid(model.alpha_raw))\n    loss = nll + 0.01*kl_alpha\n    opt.zero_grad(); loss.backward(); opt.step()",
    "Expected Result": "Across both test functions RA-DNN-MFBO attains ~5-10% lower simple regret after the same cost budget and 15-20% lower nRMSE of the high-fidelity surrogate.\nGating coefficients converge towards α≈0.3 on Levy (weak correlation) and α≈0.9 on Branin (strong correlation), confirming adaptive behaviour. Runtime overhead is negligible (<2%).",
    "Expected Conclusion": "Introducing a single learnable gate per fidelity is enough to prevent negative transfer when fidelities are weakly correlated, while leaving the original behaviour intact when correlations are strong. The change is architecturally trivial, requires only one extra parameter and a tiny KL term, yet yields consistent efficiency gains in hyper-parameter optimization. Such reliability-aware gating can be plugged into any stacked multi-fidelity neural surrogate with virtually no additional computational cost."
}

# Experimental Design
## Experiment Strategy
Goal:
Establish that Reliability-Aware DNN-MFBO (RA-DNN-MFBO) is a generally better drop-in replacement for DNN-MFBO and other multi-fidelity BO surrogates in terms of (1) search performance, (2) modelling fidelity, (3) robustness to fidelity-correlation changes, (4) computational efficiency, and (5) interpretability of the learnt gate. Every individual experiment will be an instantiation of the procedure outlined below.

1. Hypotheses to Validate
1.1 Performance: RA-DNN-MFBO reaches lower cost-weighted simple regret than baselines under the same budget.
1.2 Modelling Accuracy: The high-fidelity predictive nRMSE of RA-DNN-MFBO is lower, showing that the gate improves the surrogate itself.
1.3 Robustness: When low/high fidelity correlation is weak, adversarially non-stationary, noisy, or task-dependent, RA-DNN-MFBO avoids negative transfer while baselines suffer.
1.4 Efficiency: Added wall-clock time, GPU memory and number of parameters stay within +5% of original DNN-MFBO.
1.5 Generalization: Gains persist across (a) synthetic benchmarks, (b) public black-box optimisation suites, and (c) at least one applied industrial task.
1.6 Interpretability: Learnt αm values correlate with empirical inter-fidelity correlation coefficients; values shrink where correlation is low.

2. Comparative Landscape
2.1 Core Baselines
  • Original DNN-MFBO (hard-wired fm-1).  
  • State-of-the-art GP-based MFBO (e.g. MF-GP-EI) to show competitiveness beyond the DNN family.  
2.2 Ablations (inside RA-DNN-MFBO)  
  • Fixed αm=1 (identical to baseline)  
  • Fixed αm=0 (no low-fidelity input)  
  • Learnable αm but no KL-regularisation  
  • Per-dimension gates vs single scalar gate (complexity control).
2.3 Drop-in Test: Replace the surrogate in an existing BO pipeline (e.g. Bayesian optimisation library) with RA-DNN-MFBO and compare end-to-end quality.

3. Experimental Angles & Metrics
3.1 Quantitative
  • Cost-weighted simple regret curve (primary).  
  • Area-under-curve (AUC) of regret.  
  • nRMSE / NLL on held-out high-fidelity points.  
  • Wall-clock time, FLOPs and peak GPU memory.  
  • Variance across ≥10 random seeds; report mean±95% CI.
3.2 Qualitative / Diagnostic
  • Evolution of αm during optimisation vs estimated fidelity-correlation.  
  • Visualisations of posterior mean & variance slices to show reduced pathological extrapolation.  
  • Acquisition heat-maps showing fewer misleading low-cost queries under weak correlation.
3.3 Stress/Robustness
  • Correlation sweep: For synthetic tasks continuously morph the low-fidelity function; measure performance break-points.  
  • Noise injection: Add controlled Gaussian noise to fm-1(x).  
  • Fidelity shift: Mid-run swap low-fidelity function; examine recovery speed.

4. Unified Protocol per Experiment
Step-A   Select task/problem (with known cost model).  
Step-B   Specify budgets (cost or wall-clock) and identical initial design.  
Step-C   Run RA-DNN-MFBO + all baselines/ablations for N_seeds seeds.  
Step-D   Log metrics in real time; snapshot models every K BO steps for later analysis.  
Step-E   Post-process: compute statistics, significance tests (paired t-test/Wilcoxon, Holm correction).  
Step-F   Generate plots/tables; archive raw logs for reproducibility.

5. Environment & Reproducibility Controls
  • Hardware: Single NVIDIA A100-80GB, 2048 GB RAM node; enforce identical PyTorch/cuDNN versions.
  • Deterministic seeds, torch.backends.cudnn.deterministic=True.  
  • Automatic mixed precision OFF to keep numerical paths identical.  
  • Use WandB/MLFlow for central logging; store config & git commit hash.  
  • Provide container (Docker/Singularity) with all dependencies.

6. Success Criteria
  • At least 5% lower final simple regret AND ≥10% better AUC on ≥70% of benchmark tasks, statistically significant (p<0.05).  
  • nRMSE improvement ≥10% on ≥70% tasks.  
  • Runtime/memory overhead ≤5%.  
  • Under correlation-sweep experiments, RA curve stays within 10% of optimal (oracle choosing α per task) while baseline degrades ≥20%.  
  • αm values exhibit Pearson r>0.6 with empirical correlation estimates.

7. Multi-Perspective Demonstration
  • Algorithmic effectiveness: regret & accuracy.  
  • Practical efficiency: overhead metrics.  
  • Adaptivity & interpretability: gate behaviour.  
  • Broad applicability: synthetic → standard BO benchmarks → real application.  
  • Robustness: stress tests.

This unified experimental strategy ensures every subsequent experiment consistently interrogates the same core claims, produces comparable evidence across domains, and collectively validates RA-DNN-MFBO as a minimal yet powerful upgrade over existing multi-fidelity surrogates.

# Generated Base Code Files
{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# ------------------------------- Evaluation/visualisation ------------------------- #\n\n\ndef collect_results(results_root: Path):\n    summaries = []\n    for run_dir in results_root.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as f:\n                data = json.load(f)\n                summaries.append({\n                    \"run_id\": data[\"run_id\"],\n                    \"best_val_nrmse\": data[\"best_val_nrmse\"],\n                    \"test_nrmse\": data[\"test_nrmse\"],\n                })\n    return pd.DataFrame(summaries)\n\n\ndef generate_comparison_figures(df: pd.DataFrame, results_dir: Path):\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    sns.set(style=\"whitegrid\", font_scale=1.4)\n\n    # 1. Bar chart of test nRMSE\n    plt.figure(figsize=(8, 5))\n    ax = sns.barplot(x=\"run_id\", y=\"test_nrmse\", data=df, palette=\"viridis\")\n    for i, p in enumerate(ax.patches):\n        val = df.loc[i, \"test_nrmse\"]\n        ax.annotate(f\"{val:.3f}\", (p.get_x() + p.get_width() / 2., val), ha=\u0027center\u0027, va=\u0027bottom\u0027)\n    plt.ylabel(\"Test nRMSE (lower is better)\")\n    plt.title(\"Test nRMSE comparison across runs\")\n    plt.tight_layout()\n    plt.savefig(images_dir / \"test_nrmse_comparison.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # 2. Sorted line plot for clarity (optional)\n    df_sorted = df.sort_values(\"test_nrmse\")\n    plt.figure(figsize=(8, 5))\n    plt.plot(df_sorted[\"run_id\"], df_sorted[\"test_nrmse\"], marker=\"o\")\n    for i, val in enumerate(df_sorted[\"test_nrmse\"].values):\n        plt.annotate(f\"{val:.3f}\", (i, val))\n    plt.xlabel(\"Run\")\n    plt.ylabel(\"Test nRMSE\")\n    plt.title(\"Test nRMSE per run (sorted)\")\n    plt.tight_layout()\n    plt.savefig(images_dir / \"test_nrmse_sorted.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# ------------------------------ Main entry-point ---------------------------------- #\n\n\ndef main(results_root: str):\n    root = Path(results_root)\n    df = collect_results(root)\n    generate_comparison_figures(df, root)\n\n    # Print summary JSON to stdout\n    print(df.to_json(orient=\"records\"))\n\n\n# ---------------------------------------------------------------------------------- #\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Aggregate and visualise results across runs.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(args.results_dir)", "full_experiment_yaml": "# PLACEHOLDER: Full experiment configuration \u2013 will be populated in the next phase.\n# The structure mirrors smoke_test.yaml but with real datasets, full training budgets,\n# and all baselines/ablations.\n\nrun_variations:\n  - run_id: BASELINE_PLACEHOLDER\n    model_name: MODEL_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: \"...\"\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 123\n\n  # Additional experiment variations will be appended here during instantiation.\n", "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport yaml\n\n# ---------------------------------------------------------------------------------- #\n#                           Helper utilities for logging                            #\n# ---------------------------------------------------------------------------------- #\n\ndef tee_stream(stream, log_file):\n    \"\"\"Forward a stream to both stdout/stderr and a log file.\"\"\"\n\n    def _forward():\n        for line in iter(stream.readline, \"\"):\n            if line:\n                log_file.write(line)\n                log_file.flush()\n                print(line, end=\"\")\n        stream.close()\n\n    t = threading.Thread(target=_forward)\n    t.daemon = True\n    t.start()\n    return t\n\n\n# ---------------------------------------------------------------------------------- #\n#                                  Main logic                                      #\n# ---------------------------------------------------------------------------------- #\n\ndef run_experiments(config_path: Path, results_root: Path):\n    with open(config_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    run_variations = cfg[\"run_variations\"]\n\n    for run_cfg in run_variations:\n        run_id = run_cfg[\"run_id\"]\n        print(f\"\\n========== Running {run_id} ==========\")\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        (run_dir / \"images\").mkdir(exist_ok=True)\n\n        # Save human-readable config\n        with open(run_dir / \"config.yaml\", \"w\") as f:\n            yaml.safe_dump(run_cfg, f)\n        # Save JSON config for train subprocess\n        json_cfg_path = run_dir / \"config.json\"\n        with open(json_cfg_path, \"w\") as f:\n            json.dump(run_cfg, f)\n\n        # Build subprocess command\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--run-config\", str(json_cfg_path), \"--results-dir\", str(run_dir)]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n        # Tee stdout/stderr\n        stdout_log = open(run_dir / \"stdout.log\", \"w\")\n        stderr_log = open(run_dir / \"stderr.log\", \"w\")\n        threads = [\n            tee_stream(proc.stdout, stdout_log),\n            tee_stream(proc.stderr, stderr_log),\n        ]\n\n        proc.wait()\n        for t in threads:\n            t.join()\n        stdout_log.close()\n        stderr_log.close()\n\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess for {run_id} failed with exit code {proc.returncode}.\")\n\n    # After all runs, aggregate\n    print(\"\\n========== Aggregating results ==========\")\n    subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)], check=True)\n\n\n# ---------------------------------------------------------------------------------- #\n#                                      CLI                                         #\n# ---------------------------------------------------------------------------------- #\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Master orchestrator for experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run lightweight smoke test variations.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to save all results \u0026 figures.\")\n    args = parser.parse_args()\n\n    root = Path(args.results_dir)\n    root.mkdir(parents=True, exist_ok=True)\n\n    if args.smoke_test:\n        config_path = Path(\"config\") / \"smoke_test.yaml\"\n    else:\n        config_path = Path(\"config\") / \"full_experiment.yaml\"\n\n    run_experiments(config_path, root)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"Model architectures used across experimental variations.\n\nIncludes baseline DNN-MFBO and Reliability-Aware (RA-) DNN-MFBO. The high-fidelity\nsub-network always receives the low-fidelity prediction *mean* (not ground-truth) as\nan additional input feature. RA-DNN-MFBO learns a gating scalar to re-weight that\nfeature and includes a KL term in the loss (handled in train.py).\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# ----------------------------------------------------------------------------- #\n#                           Shared building blocks                              #\n# ----------------------------------------------------------------------------- #\n\n\nclass FidelityBlock(nn.Module):\n    def __init__(self, in_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n        # Homoscedastic noise; variance constrained positive via softplus\n        self.log_var = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        mean = self.net(x)\n        var = torch.exp(self.log_var) + 1e-6\n        return mean, var\n\n\n# ----------------------------------------------------------------------------- #\n#                            Baseline DNN-MFBO                                   #\n# ----------------------------------------------------------------------------- #\n\n\nclass DNNMFBO(nn.Module):\n    \"\"\"Two-fidelity DNN-MFBO surrogate (baseline).\"\"\"\n\n    def __init__(self, x_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim, hidden_dim)\n        self.f2 = FidelityBlock(x_dim + 1, hidden_dim)\n\n    def forward_f1(self, x):\n        return self.f1(x)\n\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1, _ = self.f1(x)\n        inp = torch.cat([x, mean1], dim=-1)\n        return self.f2(inp)\n\n    def forward(self, x, fidelity: int):\n        if fidelity == 0:\n            return self.forward_f1(x)\n        else:\n            return self.forward_f2(x)\n\n\n# ----------------------------------------------------------------------------- #\n#                         Reliability-Aware DNN-MFBO                             #\n# ----------------------------------------------------------------------------- #\n\n\nclass RADNNMFBO(nn.Module):\n    \"\"\"Reliability-Aware DNN-MFBO with a learnable gating scalar \u03b1 \u2208 (0,1).\"\"\"\n\n    def __init__(self, x_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim, hidden_dim)\n        self.alpha_raw = nn.Parameter(torch.zeros(1))  # logit-space parameter\n        self.f2 = FidelityBlock(x_dim + 1, hidden_dim)\n\n    def forward_f1(self, x):\n        return self.f1(x)\n\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1, _ = self.f1(x)\n        alpha = torch.sigmoid(self.alpha_raw)\n        inp = torch.cat([x, alpha * mean1], dim=-1)\n        return self.f2(inp)\n\n    def forward(self, x, fidelity: int):\n        if fidelity == 0:\n            return self.forward_f1(x)\n        else:\n            return self.forward_f2(x)\n\n\n# ----------------------------------------------------------------------------- #\n#                               Ablation placeholder                             #\n# ----------------------------------------------------------------------------- #\n\n\nclass ABLATION_PLACEHOLDER(nn.Module):\n    \"\"\"PLACEHOLDER: Will be replaced with ablation model implementations.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        raise NotImplementedError(\"This ablation model is a placeholder.\")\n\n\n# ----------------------------------------------------------------------------- #\n#                               Factory function                                 #\n# ----------------------------------------------------------------------------- #\n\ndef build_model(model_name: str, x_dim: int, model_cfg: Dict | None = None):\n    model_cfg = model_cfg or {}\n    name = model_name.lower()\n    if name in {\"dnn_mfbo\", \"baseline\"}:\n        return DNNMFBO(x_dim=x_dim, **model_cfg)\n    elif name in {\"radnn\", \"radnn_mfbo\", \"ra_dnn_mfbo\"}:\n        return RADNNMFBO(x_dim=x_dim, **model_cfg)\n    elif name.startswith(\"ablation\"):\n        return ABLATION_PLACEHOLDER()\n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")", "preprocess_py": "\"\"\"Dataset loading and common preprocessing utilities.\n\nAll experimental variations *must* rely on this file for data access so that preprocessing\nremains identical across runs.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# ----------------------------------------------------------------------------- #\n#                               Synthetic datasets                              #\n# ----------------------------------------------------------------------------- #\n\n\ndef _branin(x: torch.Tensor) -\u003e torch.Tensor:\n    x1, x2 = x[..., 0], x[..., 1]\n    a = 1.0\n    b = 5.1 / (4 * math.pi ** 2)\n    c = 5 / math.pi\n    r = 6.0\n    s = 10.0\n    t = 1 / (8 * math.pi)\n    return a * (x2 - b * x1 ** 2 + c * x1 - r) ** 2 + s * (1 - t) * torch.cos(x1) + s\n\n\ndef _low_fidelity_branin(x: torch.Tensor) -\u003e torch.Tensor:\n    # Simple smooth transformation to emulate low-fidelity artefacts\n    return _branin(x) * 0.5 + 10 * torch.sin(x[..., 0])\n\n\nclass SyntheticFunctionDataset(Dataset):\n    def __init__(self, n_samples: int, fidelity: int, function_name: str = \"branin\", noise_std: float = 0.0):\n        super().__init__()\n        self.fidelity = fidelity  # 0 = low, 1 = high\n        self.function_name = function_name\n        self.noise_std = noise_std\n\n        # Random sampling within domain for Branin-like problems\n        x1 = torch.FloatTensor(n_samples).uniform_(-5, 10)\n        x2 = torch.FloatTensor(n_samples).uniform_(0, 15)\n        self.x = torch.stack([x1, x2], dim=-1)\n\n        if function_name == \"branin\":\n            if fidelity == 0:\n                self.y = _low_fidelity_branin(self.x)\n            else:\n                self.y = _branin(self.x)\n        else:\n            raise ValueError(\"Unknown synthetic function: %s\" % function_name)\n\n        if noise_std \u003e 0:\n            self.y += noise_std * torch.randn_like(self.y)\n\n    def __len__(self):\n        return self.x.size(0)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.x[idx], \"y\": self.y[idx], \"fidelity\": self.fidelity}\n\n\n# ----------------------------------------------------------------------------- #\n#                               Loader facade                                   #\n# ----------------------------------------------------------------------------- #\n\ndef load_dataset(dataset_cfg: Dict, batch_size: int = 32) -\u003e Tuple[Dict, int]:\n    \"\"\"Load dataset according to configuration and return dataloaders per fidelity.\n\n    Returns a dict of the form {split: {fid: DataLoader}} and the input dimension.\n    \"\"\"\n\n    name = dataset_cfg[\"name\"].lower()\n    if name == \"synthetic\":\n        # Placeholder dataset \u2013 will be replaced with real loaders in subsequent steps\n        function_name = dataset_cfg.get(\"function\", \"branin\")\n        n_low = dataset_cfg.get(\"n_samples_low\", 200)\n        n_high = dataset_cfg.get(\"n_samples_high\", 200)\n\n        ds_low = SyntheticFunctionDataset(n_low, fidelity=0, function_name=function_name)\n        ds_high = SyntheticFunctionDataset(n_high, fidelity=1, function_name=function_name)\n\n        # Simple 80/10/10 split per fidelity\n        def split_dataset(ds):\n            n = len(ds)\n            idxs = list(range(n))\n            random.shuffle(idxs)\n            n_train = int(0.8 * n)\n            n_val = int(0.1 * n)\n            train_idx = idxs[:n_train]\n            val_idx = idxs[n_train : n_train + n_val]\n            test_idx = idxs[n_train + n_val :]\n            return (\n                torch.utils.data.Subset(ds, train_idx),\n                torch.utils.data.Subset(ds, val_idx),\n                torch.utils.data.Subset(ds, test_idx),\n            )\n\n        splits_low = split_dataset(ds_low)\n        splits_high = split_dataset(ds_high)\n\n        dataloaders = {\n            \"train\": {\n                0: DataLoader(splits_low[0], batch_size=batch_size, shuffle=True),\n                1: DataLoader(splits_high[0], batch_size=batch_size, shuffle=True),\n            },\n            \"val\": {\n                0: DataLoader(splits_low[1], batch_size=batch_size, shuffle=False),\n                1: DataLoader(splits_high[1], batch_size=batch_size, shuffle=False),\n            },\n            \"test\": {\n                0: DataLoader(splits_low[2], batch_size=batch_size, shuffle=False),\n                1: DataLoader(splits_high[2], batch_size=batch_size, shuffle=False),\n            },\n        }\n        x_dim = ds_low[0][\"x\"].numel()\n        return dataloaders, x_dim\n\n    # --------------------------------- PLACEHOLDER --------------------------------- #\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic               #\n    # -------------------------------------------------------------------------------- #\n\n    raise NotImplementedError(f\"Dataset \u0027{name}\u0027 is not implemented.\")", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ra-dnn-mfbo-framework\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for RA-DNN-MFBO experiments\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n  \"torch\u003e=2.0.0\",\n  \"numpy\u003e=1.23\",\n  \"pyyaml\u003e=6.0\",\n  \"matplotlib\u003e=3.6\",\n  \"seaborn\u003e=0.12\",\n  \"pandas\u003e=1.5\",\n  \"scikit-learn\u003e=1.2\",\n  \"tqdm\u003e=4.65\"\n]\n", "smoke_test_yaml": "# Lightweight smoke-test configuration \u2013 executes quickly on CI\nrun_variations:\n  - run_id: baseline_smoke\n    model_name: dnn_mfbo\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    seed: 42\n\n  - run_id: radnn_smoke\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    seed: 43", "train_py": "import argparse\nimport json\nimport os\nimport random\nimport sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom src import model as model_lib\nfrom src import preprocess as preprocess_lib\n\n# ------------------------------- Utility functions -------------------------------- #\n\ndef set_deterministic(seed: int = 42):\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef nll_gaussian(y, mean, var):\n    \"\"\"Negative log-likelihood of isotropic Gaussian with diagonal variance.\"\"\"\n    return 0.5 * torch.log(var) + 0.5 * (y - mean) ** 2 / var\n\n\ndef nrmse(y_true: torch.Tensor, y_pred: torch.Tensor):\n    rmse = torch.sqrt(F.mse_loss(y_pred, y_true))\n    return (rmse / (y_true.max() - y_true.min())).item()\n\n\n# ------------------------------- Training routine --------------------------------- #\n\n\ndef train_single_run(run_config: dict, results_dir: Path):\n    results_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    # 1. Deterministic behaviour\n    seed = run_config.get(\"seed\", 42)\n    set_deterministic(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 2. Data\n    dataloaders, x_dim = preprocess_lib.load_dataset(run_config[\"dataset\"], batch_size=run_config[\"training\"][\"batch_size\"])\n\n    # 3. Model\n    model = model_lib.build_model(run_config[\"model_name\"], x_dim=x_dim, model_cfg=run_config.get(\"model\", {})).to(device)\n\n    # 4. Optimiser \u0026 misc\n    lr = run_config[\"training\"].get(\"lr\", 1e-3)\n    epochs = run_config[\"training\"].get(\"epochs\", 100)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    train_loss_hist, val_loss_hist, val_nrmse_hist = [], [], []\n    best_val_nrmse = float(\"inf\")\n\n    for epoch in tqdm(range(1, epochs + 1), desc=f\"{run_config[\u0027run_id\u0027]}\"):\n        model.train()\n        epoch_losses = []\n\n        # --- iterate low then high fidelity to guarantee low-forward availability ---\n        for fidelity in [0, 1]:\n            for batch in dataloaders[\"train\"][fidelity]:\n                x = batch[\"x\"].to(device)\n                y = batch[\"y\"].to(device)\n\n                if fidelity == 0:\n                    mean, var = model.forward_f1(x)\n                    loss = nll_gaussian(y, mean, var).mean()\n                else:  # high fidelity\n                    mean, var = model.forward_f2(x)\n                    loss = nll_gaussian(y, mean, var).mean()\n                    # Add KL on alpha if present (RADNN)\n                    if hasattr(model, \"alpha_raw\"):\n                        alpha = torch.sigmoid(model.alpha_raw)\n                        kl_alpha = -torch.distributions.Beta(1.0, 1.0).log_prob(alpha)\n                        loss = loss + run_config.get(\"kl_coeff\", 0.01) * kl_alpha\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                epoch_losses.append(loss.item())\n\n        train_loss_hist.append(np.mean(epoch_losses))\n\n        # ---------------- Validation ----------------\n        model.eval()\n        with torch.no_grad():\n            val_losses, preds, targets = [], [], []\n            for batch in dataloaders[\"val\"][1]:  # high fidelity only for val metrics\n                x = batch[\"x\"].to(device)\n                y = batch[\"y\"].to(device)\n                mean, var = model.forward_f2(x)\n                loss = nll_gaussian(y, mean, var).mean()\n                val_losses.append(loss.item())\n                preds.append(mean.cpu())\n                targets.append(y.cpu())\n        val_loss = np.mean(val_losses)\n        val_loss_hist.append(val_loss)\n\n        preds = torch.cat(preds, dim=0)\n        targets = torch.cat(targets, dim=0)\n        val_nrmse_val = nrmse(targets, preds)\n        val_nrmse_hist.append(val_nrmse_val)\n        best_val_nrmse = min(best_val_nrmse, val_nrmse_val)\n\n    # ---------------- Testing ----------------\n    model.eval()\n    with torch.no_grad():\n        preds, targets = [], []\n        for batch in dataloaders[\"test\"][1]:\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            mean, _ = model.forward_f2(x)\n            preds.append(mean.cpu())\n            targets.append(y.cpu())\n    preds = torch.cat(preds, dim=0)\n    targets = torch.cat(targets, dim=0)\n    test_nrmse_val = nrmse(targets, preds)\n\n    # ---------------- Save artefacts ----------------\n    torch.save(model.state_dict(), results_dir / \"model.pt\")\n\n    metrics = {\n        \"run_id\": run_config[\"run_id\"],\n        \"best_val_nrmse\": best_val_nrmse,\n        \"test_nrmse\": test_nrmse_val,\n        \"train_loss_hist\": train_loss_hist,\n        \"val_loss_hist\": val_loss_hist,\n        \"val_nrmse_hist\": val_nrmse_hist,\n        \"config\": run_config,\n    }\n\n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # ------------------ Figures --------------------\n    sns.set(style=\"whitegrid\", font_scale=1.3)\n\n    # 1. Training / validation loss curves\n    plt.figure(figsize=(6, 4))\n    plt.plot(train_loss_hist, label=\"Train NLL\")\n    plt.plot(val_loss_hist, label=\"Val NLL\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Negative Log-Likelihood\")\n    plt.title(f\"Training Curve \u2013 {run_config[\u0027run_id\u0027]}\")\n    plt.legend()\n    plt.annotate(f\"Final Val = {val_loss_hist[-1]:.3f}\", xy=(len(val_loss_hist) - 1, val_loss_hist[-1]))\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"training_loss_{run_config[\u0027run_id\u0027]}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # 2. nRMSE curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(val_nrmse_hist, label=\"Val nRMSE\", color=\"green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"nRMSE\")\n    plt.title(f\"Validation nRMSE \u2013 {run_config[\u0027run_id\u0027]}\")\n    plt.annotate(f\"Best = {best_val_nrmse:.3f}\", xy=(np.argmin(val_nrmse_hist), best_val_nrmse))\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"nrmse_{run_config[\u0027run_id\u0027]}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Print JSON summary to stdout for structured logging\n    print(json.dumps({\"run_id\": run_config[\"run_id\"], \"best_val_nrmse\": best_val_nrmse, \"test_nrmse\": test_nrmse_val}))\n\n\n# ---------------------------------------------------------------------------------- #\n\n\ndef cli_main():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to a JSON file containing the run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to write outputs to.\")\n    args = parser.parse_args()\n\n    with open(args.run_config, \"r\") as f:\n        run_cfg = json.load(f)\n\n    train_single_run(run_cfg, Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    cli_main()"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Criterion 1 violated – although train.py saves the trained network, no complementary loading routine exists anywhere in the code-base (evaluate.py only reads the pre-computed JSON and never demonstrates model re-loading).  Therefore the required \"model saving/loading mechanisms are implemented\" condition is not fully satisfied.  All other criteria are met, but the absence of a loading path means the core foundation is incomplete."
}
