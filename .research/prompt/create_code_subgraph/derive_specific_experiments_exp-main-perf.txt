
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['baseline-dnn-mfbo', 'ra-dnn-mfbo', 'ra-dnn-mfbo-noKL', 'ra-dnn-fixed-alpha1', 'ra-dnn-fixed-alpha0']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "DNN-MFBO hard-wires the previous-fidelity output fm-1(x) as an unscaled input feature for fm(x).\nIf the correlation between fidelities is weak or changes locally, this unconditional use of fm-1(x) can introduce negative transfer, slowing hyper-parameter search and occasionally selecting misleading low-cost queries. A minimal mechanism is needed to adaptively reduce or amplify the influence of lower fidelities without redesigning the architecture or acquisition.",
    "Methods": "Reliability-Aware DNN-MFBO (RA-DNN-MFBO)\nMinimal change: insert one learnable scalar gate αm∈[0,1] for every fidelity m>1.\nInput becomes   xm=[x ; αm·fm-1(x)]   instead of   [x ; fm-1(x)].\nPrior: αm∼Beta(1,1)  (uniform). In variational inference we keep a Gaussian variational posterior on logit(αm). An extra KL( q(αm) || p(αm) ) term is added to the ELBO (two additional lines of code).\nMotivation:\n• If fm-1 is unreliable, the posterior pushes αm→0, avoiding negative transfer.\n• When fidelities are strongly correlated the network learns αm→1, recovering the original model.\n• This single multiplicative gate is differentiable, cheap, and does not change acquisition-function derivations (moment matching still applies because scaling a Gaussian keeps it Gaussian).",
    "Experimental Setup": "Data: 2-fidelity Branin and Levy benchmarks supplied with DNN-MFBO code (low-fidelity = cheap interpolation, high-fidelity = ground truth).\nBaselines: Original DNN-MFBO vs proposed RA-DNN-MFBO.\nInitial design: 10 random queries per fidelity.\nBudget: cost-weighted budget of 150 (low-fidelity cost 1, high-fidelity cost 10).\nMetrics: simple regret w.r.t. cost, plus nRMSE of surrogate on 100 test points to isolate modelling effect.\nRepeats: 10 random seeds.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass FidelityBlock(nn.Module):\n    def __init__(self, in_dim, hid=50):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(in_dim, hid), nn.ReLU(),\n                                 nn.Linear(hid, 1))\n        self.log_var = nn.Parameter(torch.zeros(1))  # homoscedastic noise\n    def forward(self, x):\n        mean = self.net(x)\n        var  = torch.exp(self.log_var)\n        return mean, var\n\nclass RADNNMFBO(nn.Module):\n    \"\"\"2-fidelity version for demo\"\"\"\n    def __init__(self, x_dim):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim)\n        self.alpha_raw = nn.Parameter(torch.zeros(1))  # sigmoid→α∈(0,1)\n        self.f2 = FidelityBlock(x_dim+1)\n    def forward_f1(self, x):\n        return self.f1(x)\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1,_ = self.f1(x)\n        alpha = torch.sigmoid(self.alpha_raw)\n        inp = torch.cat([x, alpha*mean1], dim=-1)\n        return self.f2(inp)\n# --- tiny training loop for surrogate only ---\nopt = torch.optim.Adam(model.parameters(), 1e-3)\nfor itr in range(1000):\n    m1,v1 = model.forward_f1(x1);  m2,v2 = model.forward_f2(x2)\n    nll  = ((y1-m1)**2/v1 + torch.log(v1)).mean()\n    nll += ((y2-m2)**2/v2 + torch.log(v2)).mean()\n    # Beta(1,1) prior KL on α (Gaussian approx):\n    kl_alpha = -torch.distributions.Beta(1,1).log_prob(torch.sigmoid(model.alpha_raw))\n    loss = nll + 0.01*kl_alpha\n    opt.zero_grad(); loss.backward(); opt.step()",
    "Expected Result": "Across both test functions RA-DNN-MFBO attains ~5-10% lower simple regret after the same cost budget and 15-20% lower nRMSE of the high-fidelity surrogate.\nGating coefficients converge towards α≈0.3 on Levy (weak correlation) and α≈0.9 on Branin (strong correlation), confirming adaptive behaviour. Runtime overhead is negligible (<2%).",
    "Expected Conclusion": "Introducing a single learnable gate per fidelity is enough to prevent negative transfer when fidelities are weakly correlated, while leaving the original behaviour intact when correlations are strong. The change is architecturally trivial, requires only one extra parameter and a tiny KL term, yet yields consistent efficiency gains in hyper-parameter optimization. Such reliability-aware gating can be plugged into any stacked multi-fidelity neural surrogate with virtually no additional computational cost."
}

# Experimental Design
- Strategy: Goal:
Establish that Reliability-Aware DNN-MFBO (RA-DNN-MFBO) is a generally better drop-in replacement for DNN-MFBO and other multi-fidelity BO surrogates in terms of (1) search performance, (2) modelling fidelity, (3) robustness to fidelity-correlation changes, (4) computational efficiency, and (5) interpretability of the learnt gate. Every individual experiment will be an instantiation of the procedure outlined below.

1. Hypotheses to Validate
1.1 Performance: RA-DNN-MFBO reaches lower cost-weighted simple regret than baselines under the same budget.
1.2 Modelling Accuracy: The high-fidelity predictive nRMSE of RA-DNN-MFBO is lower, showing that the gate improves the surrogate itself.
1.3 Robustness: When low/high fidelity correlation is weak, adversarially non-stationary, noisy, or task-dependent, RA-DNN-MFBO avoids negative transfer while baselines suffer.
1.4 Efficiency: Added wall-clock time, GPU memory and number of parameters stay within +5% of original DNN-MFBO.
1.5 Generalization: Gains persist across (a) synthetic benchmarks, (b) public black-box optimisation suites, and (c) at least one applied industrial task.
1.6 Interpretability: Learnt αm values correlate with empirical inter-fidelity correlation coefficients; values shrink where correlation is low.

2. Comparative Landscape
2.1 Core Baselines
  • Original DNN-MFBO (hard-wired fm-1).  
  • State-of-the-art GP-based MFBO (e.g. MF-GP-EI) to show competitiveness beyond the DNN family.  
2.2 Ablations (inside RA-DNN-MFBO)  
  • Fixed αm=1 (identical to baseline)  
  • Fixed αm=0 (no low-fidelity input)  
  • Learnable αm but no KL-regularisation  
  • Per-dimension gates vs single scalar gate (complexity control).
2.3 Drop-in Test: Replace the surrogate in an existing BO pipeline (e.g. Bayesian optimisation library) with RA-DNN-MFBO and compare end-to-end quality.

3. Experimental Angles & Metrics
3.1 Quantitative
  • Cost-weighted simple regret curve (primary).  
  • Area-under-curve (AUC) of regret.  
  • nRMSE / NLL on held-out high-fidelity points.  
  • Wall-clock time, FLOPs and peak GPU memory.  
  • Variance across ≥10 random seeds; report mean±95% CI.
3.2 Qualitative / Diagnostic
  • Evolution of αm during optimisation vs estimated fidelity-correlation.  
  • Visualisations of posterior mean & variance slices to show reduced pathological extrapolation.  
  • Acquisition heat-maps showing fewer misleading low-cost queries under weak correlation.
3.3 Stress/Robustness
  • Correlation sweep: For synthetic tasks continuously morph the low-fidelity function; measure performance break-points.  
  • Noise injection: Add controlled Gaussian noise to fm-1(x).  
  • Fidelity shift: Mid-run swap low-fidelity function; examine recovery speed.

4. Unified Protocol per Experiment
Step-A   Select task/problem (with known cost model).  
Step-B   Specify budgets (cost or wall-clock) and identical initial design.  
Step-C   Run RA-DNN-MFBO + all baselines/ablations for N_seeds seeds.  
Step-D   Log metrics in real time; snapshot models every K BO steps for later analysis.  
Step-E   Post-process: compute statistics, significance tests (paired t-test/Wilcoxon, Holm correction).  
Step-F   Generate plots/tables; archive raw logs for reproducibility.

5. Environment & Reproducibility Controls
  • Hardware: Single NVIDIA A100-80GB, 2048 GB RAM node; enforce identical PyTorch/cuDNN versions.
  • Deterministic seeds, torch.backends.cudnn.deterministic=True.  
  • Automatic mixed precision OFF to keep numerical paths identical.  
  • Use WandB/MLFlow for central logging; store config & git commit hash.  
  • Provide container (Docker/Singularity) with all dependencies.

6. Success Criteria
  • At least 5% lower final simple regret AND ≥10% better AUC on ≥70% of benchmark tasks, statistically significant (p<0.05).  
  • nRMSE improvement ≥10% on ≥70% tasks.  
  • Runtime/memory overhead ≤5%.  
  • Under correlation-sweep experiments, RA curve stays within 10% of optimal (oracle choosing α per task) while baseline degrades ≥20%.  
  • αm values exhibit Pearson r>0.6 with empirical correlation estimates.

7. Multi-Perspective Demonstration
  • Algorithmic effectiveness: regret & accuracy.  
  • Practical efficiency: overhead metrics.  
  • Adaptivity & interpretability: gate behaviour.  
  • Broad applicability: synthetic → standard BO benchmarks → real application.  
  • Robustness: stress tests.

This unified experimental strategy ensures every subsequent experiment consistently interrogates the same core claims, produces comparable evidence across domains, and collectively validates RA-DNN-MFBO as a minimal yet powerful upgrade over existing multi-fidelity surrogates.

# Current Experiment (to generate code for)
- Experiment ID: exp-main-perf
- Description: Objective / Hypothesis:
Validate H1 (performance) and H2 (modelling accuracy) while checking H4 (efficiency).  The proposed Reliability-Aware gate should (1) lower cost-weighted simple regret and (2) reduce nRMSE of the high-fidelity surrogate with ≤5 % compute overhead.

Benchmarks & Datasets:
• Synthetic 2-fidelity Branin & Levy (exact definition in DNN-MFBO repo)  
• Black-box-optimisation suite BBOB-MF (Surrogates 101, 12 functions × 2 fidelities)  
• NAS-Bench-201-MF (epoch-truncated CIFAR-10 training costs: 4, 12, 200 epochs → collapse to two fidelities: 4 vs 200)  

Pre-processing:
• Normalise continuous input dimensions to [0,1]  
• Min-max normalise low- & high-fidelity outputs separately  
• Log-transform cost values, then scale to mean 0, std 1 before passing to acquisition  

Data Splitting:
BO automatically generates data; hold-out test sets of 100 Latin-Hypercube points per function for surrogate nRMSE/NLL.  

Number of repetitions:
10 seeds per benchmark. Report mean±95 % CI. Surrogate snapshot every 5 BO steps.  

Evaluation Metrics:
Primary: cost-weighted simple regret @ budget 150 (synthetic) / 3 × true-high-fidelity cost (BBOB-MF) / 8 GPU-hours (NAS).  Secondary: AUC of regret curve, nRMSE, NLL, wall-clock time, peak GPU memory, FLOPs (fvcore), #parameters.  

Comparisons inside experiment:
1. baseline-dnn-mfbo – original hard-wired model  
2. ra-dnn-mfbo – proposed learnt gate + KL  
3. ra-dnn-mfbo-noKL – drop KL term to test regularisation importance  
4. ra-dnn-fixed-alpha1 – force α=1 (identical to baseline feature-wise)  
5. ra-dnn-fixed-alpha0 – remove low-fidelity feature entirely  
External reference (plotted but not in run_variations): MF-GP-EI from BoTorch, using default hyper-priors.  

Hyper-parameter Grid (analysed post-hoc): learning-rate ∈{1e-4,3e-4,1e-3}; network width {32,64}; KL-weight β∈{0.005,0.01,0.05}. Swept on Branin-low-budget via 3-fold CV; best setting applied across tasks, sensitivity curves reported.  

Robustness within experiment: not explicit – handled in exp-robustness.  

Efficiency Measurement:
• Use nvprof to capture FLOPs and kernel time per BO iteration (average of middle 50 % iterations)  
• Measure peak GPU memory with torch.cuda.max_memory_allocated()  
• Wall-clock recorded via Python’s time module, excluding plotting.  

Selection Criterion for plots:
• Best-validation log-likelihood model per seed (early stopping patience=30 iterations)  

Example Code Snippet:
```
from mfbo import make_problem
from radnn import RADNNMFBO, DNNSurrogate
from utils import bo_loop

task = make_problem("branin", n_fidelities=2)
model = RADNNMFBO(task.x_dim) if variant=="ra-dnn-mfbo" else DNNSurrogate(...)
results = bo_loop(model, task, budget=150, seed=seed)
```
Outcome Expected: ≥10 % lower final regret and AUC on ≥70 % of problems, ≥15 % lower nRMSE, ≤2 % run-time overhead.

- Run Variations: ['baseline-dnn-mfbo', 'ra-dnn-mfbo', 'ra-dnn-mfbo-noKL', 'ra-dnn-fixed-alpha1', 'ra-dnn-fixed-alpha0']

# Base Code
{'train_py': 'import argparse\nimport json\nimport os\nimport random\nimport sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom src import model as model_lib\nfrom src import preprocess as preprocess_lib\n\n# ------------------------------- Utility functions -------------------------------- #\n\ndef set_deterministic(seed: int = 42):\n    """Set random seeds for reproducibility."""\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef nll_gaussian(y, mean, var):\n    """Negative log-likelihood of isotropic Gaussian with diagonal variance."""\n    return 0.5 * torch.log(var) + 0.5 * (y - mean) ** 2 / var\n\n\ndef nrmse(y_true: torch.Tensor, y_pred: torch.Tensor):\n    rmse = torch.sqrt(F.mse_loss(y_pred, y_true))\n    return (rmse / (y_true.max() - y_true.min())).item()\n\n\n# ------------------------------- Training routine --------------------------------- #\n\n\ndef train_single_run(run_config: dict, results_dir: Path):\n    results_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = results_dir / "images"\n    images_dir.mkdir(exist_ok=True)\n\n    # 1. Deterministic behaviour\n    seed = run_config.get("seed", 42)\n    set_deterministic(seed)\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    # 2. Data\n    dataloaders, x_dim = preprocess_lib.load_dataset(run_config["dataset"], batch_size=run_config["training"]["batch_size"])\n\n    # 3. Model\n    model = model_lib.build_model(run_config["model_name"], x_dim=x_dim, model_cfg=run_config.get("model", {})).to(device)\n\n    # 4. Optimiser & misc\n    lr = run_config["training"].get("lr", 1e-3)\n    epochs = run_config["training"].get("epochs", 100)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    train_loss_hist, val_loss_hist, val_nrmse_hist = [], [], []\n    best_val_nrmse = float("inf")\n\n    for epoch in tqdm(range(1, epochs + 1), desc=f"{run_config[\'run_id\']}"):\n        model.train()\n        epoch_losses = []\n\n        # --- iterate low then high fidelity to guarantee low-forward availability ---\n        for fidelity in [0, 1]:\n            for batch in dataloaders["train"][fidelity]:\n                x = batch["x"].to(device)\n                y = batch["y"].to(device)\n\n                if fidelity == 0:\n                    mean, var = model.forward_f1(x)\n                    loss = nll_gaussian(y, mean, var).mean()\n                else:  # high fidelity\n                    mean, var = model.forward_f2(x)\n                    loss = nll_gaussian(y, mean, var).mean()\n                    # Add KL on alpha if present (RADNN)\n                    if hasattr(model, "alpha_raw"):\n                        alpha = torch.sigmoid(model.alpha_raw)\n                        kl_alpha = -torch.distributions.Beta(1.0, 1.0).log_prob(alpha)\n                        loss = loss + run_config.get("kl_coeff", 0.01) * kl_alpha\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                epoch_losses.append(loss.item())\n\n        train_loss_hist.append(np.mean(epoch_losses))\n\n        # ---------------- Validation ----------------\n        model.eval()\n        with torch.no_grad():\n            val_losses, preds, targets = [], [], []\n            for batch in dataloaders["val"][1]:  # high fidelity only for val metrics\n                x = batch["x"].to(device)\n                y = batch["y"].to(device)\n                mean, var = model.forward_f2(x)\n                loss = nll_gaussian(y, mean, var).mean()\n                val_losses.append(loss.item())\n                preds.append(mean.cpu())\n                targets.append(y.cpu())\n        val_loss = np.mean(val_losses)\n        val_loss_hist.append(val_loss)\n\n        preds = torch.cat(preds, dim=0)\n        targets = torch.cat(targets, dim=0)\n        val_nrmse_val = nrmse(targets, preds)\n        val_nrmse_hist.append(val_nrmse_val)\n        best_val_nrmse = min(best_val_nrmse, val_nrmse_val)\n\n    # ---------------- Testing ----------------\n    model.eval()\n    with torch.no_grad():\n        preds, targets = [], []\n        for batch in dataloaders["test"][1]:\n            x, y = batch["x"].to(device), batch["y"].to(device)\n            mean, _ = model.forward_f2(x)\n            preds.append(mean.cpu())\n            targets.append(y.cpu())\n    preds = torch.cat(preds, dim=0)\n    targets = torch.cat(targets, dim=0)\n    test_nrmse_val = nrmse(targets, preds)\n\n    # ---------------- Save artefacts ----------------\n    torch.save(model.state_dict(), results_dir / "model.pt")\n\n    metrics = {\n        "run_id": run_config["run_id"],\n        "best_val_nrmse": best_val_nrmse,\n        "test_nrmse": test_nrmse_val,\n        "train_loss_hist": train_loss_hist,\n        "val_loss_hist": val_loss_hist,\n        "val_nrmse_hist": val_nrmse_hist,\n        "config": run_config,\n    }\n\n    with open(results_dir / "results.json", "w") as f:\n        json.dump(metrics, f, indent=2)\n\n    # ------------------ Figures --------------------\n    sns.set(style="whitegrid", font_scale=1.3)\n\n    # 1. Training / validation loss curves\n    plt.figure(figsize=(6, 4))\n    plt.plot(train_loss_hist, label="Train NLL")\n    plt.plot(val_loss_hist, label="Val NLL")\n    plt.xlabel("Epoch")\n    plt.ylabel("Negative Log-Likelihood")\n    plt.title(f"Training Curve – {run_config[\'run_id\']}")\n    plt.legend()\n    plt.annotate(f"Final Val = {val_loss_hist[-1]:.3f}", xy=(len(val_loss_hist) - 1, val_loss_hist[-1]))\n    plt.tight_layout()\n    plt.savefig(images_dir / f"training_loss_{run_config[\'run_id\']}.pdf", bbox_inches="tight")\n    plt.close()\n\n    # 2. nRMSE curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(val_nrmse_hist, label="Val nRMSE", color="green")\n    plt.xlabel("Epoch")\n    plt.ylabel("nRMSE")\n    plt.title(f"Validation nRMSE – {run_config[\'run_id\']}")\n    plt.annotate(f"Best = {best_val_nrmse:.3f}", xy=(np.argmin(val_nrmse_hist), best_val_nrmse))\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f"nrmse_{run_config[\'run_id\']}.pdf", bbox_inches="tight")\n    plt.close()\n\n    # Print JSON summary to stdout for structured logging\n    print(json.dumps({"run_id": run_config["run_id"], "best_val_nrmse": best_val_nrmse, "test_nrmse": test_nrmse_val}))\n\n\n# ---------------------------------------------------------------------------------- #\n\n\ndef cli_main():\n    parser = argparse.ArgumentParser(description="Run a single experiment variation.")\n    parser.add_argument("--run-config", type=str, required=True, help="Path to a JSON file containing the run configuration.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Directory to write outputs to.")\n    args = parser.parse_args()\n\n    with open(args.run_config, "r") as f:\n        run_cfg = json.load(f)\n\n    train_single_run(run_cfg, Path(args.results_dir))\n\n\nif __name__ == "__main__":\n    cli_main()', 'evaluate_py': 'import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# ------------------------------- Evaluation/visualisation ------------------------- #\n\n\ndef collect_results(results_root: Path):\n    summaries = []\n    for run_dir in results_root.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / "results.json"\n        if res_file.exists():\n            with open(res_file, "r") as f:\n                data = json.load(f)\n                summaries.append({\n                    "run_id": data["run_id"],\n                    "best_val_nrmse": data["best_val_nrmse"],\n                    "test_nrmse": data["test_nrmse"],\n                })\n    return pd.DataFrame(summaries)\n\n\ndef generate_comparison_figures(df: pd.DataFrame, results_dir: Path):\n    images_dir = results_dir / "images"\n    images_dir.mkdir(exist_ok=True)\n\n    sns.set(style="whitegrid", font_scale=1.4)\n\n    # 1. Bar chart of test nRMSE\n    plt.figure(figsize=(8, 5))\n    ax = sns.barplot(x="run_id", y="test_nrmse", data=df, palette="viridis")\n    for i, p in enumerate(ax.patches):\n        val = df.loc[i, "test_nrmse"]\n        ax.annotate(f"{val:.3f}", (p.get_x() + p.get_width() / 2., val), ha=\'center\', va=\'bottom\')\n    plt.ylabel("Test nRMSE (lower is better)")\n    plt.title("Test nRMSE comparison across runs")\n    plt.tight_layout()\n    plt.savefig(images_dir / "test_nrmse_comparison.pdf", bbox_inches="tight")\n    plt.close()\n\n    # 2. Sorted line plot for clarity (optional)\n    df_sorted = df.sort_values("test_nrmse")\n    plt.figure(figsize=(8, 5))\n    plt.plot(df_sorted["run_id"], df_sorted["test_nrmse"], marker="o")\n    for i, val in enumerate(df_sorted["test_nrmse"].values):\n        plt.annotate(f"{val:.3f}", (i, val))\n    plt.xlabel("Run")\n    plt.ylabel("Test nRMSE")\n    plt.title("Test nRMSE per run (sorted)")\n    plt.tight_layout()\n    plt.savefig(images_dir / "test_nrmse_sorted.pdf", bbox_inches="tight")\n    plt.close()\n\n\n# ------------------------------ Main entry-point ---------------------------------- #\n\n\ndef main(results_root: str):\n    root = Path(results_root)\n    df = collect_results(root)\n    generate_comparison_figures(df, root)\n\n    # Print summary JSON to stdout\n    print(df.to_json(orient="records"))\n\n\n# ---------------------------------------------------------------------------------- #\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(description="Aggregate and visualise results across runs.")\n    parser.add_argument("--results-dir", type=str, required=True)\n    args = parser.parse_args()\n    main(args.results_dir)', 'preprocess_py': '"""Dataset loading and common preprocessing utilities.\n\nAll experimental variations *must* rely on this file for data access so that preprocessing\nremains identical across runs.\n"""\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# ----------------------------------------------------------------------------- #\n#                               Synthetic datasets                              #\n# ----------------------------------------------------------------------------- #\n\n\ndef _branin(x: torch.Tensor) -> torch.Tensor:\n    x1, x2 = x[..., 0], x[..., 1]\n    a = 1.0\n    b = 5.1 / (4 * math.pi ** 2)\n    c = 5 / math.pi\n    r = 6.0\n    s = 10.0\n    t = 1 / (8 * math.pi)\n    return a * (x2 - b * x1 ** 2 + c * x1 - r) ** 2 + s * (1 - t) * torch.cos(x1) + s\n\n\ndef _low_fidelity_branin(x: torch.Tensor) -> torch.Tensor:\n    # Simple smooth transformation to emulate low-fidelity artefacts\n    return _branin(x) * 0.5 + 10 * torch.sin(x[..., 0])\n\n\nclass SyntheticFunctionDataset(Dataset):\n    def __init__(self, n_samples: int, fidelity: int, function_name: str = "branin", noise_std: float = 0.0):\n        super().__init__()\n        self.fidelity = fidelity  # 0 = low, 1 = high\n        self.function_name = function_name\n        self.noise_std = noise_std\n\n        # Random sampling within domain for Branin-like problems\n        x1 = torch.FloatTensor(n_samples).uniform_(-5, 10)\n        x2 = torch.FloatTensor(n_samples).uniform_(0, 15)\n        self.x = torch.stack([x1, x2], dim=-1)\n\n        if function_name == "branin":\n            if fidelity == 0:\n                self.y = _low_fidelity_branin(self.x)\n            else:\n                self.y = _branin(self.x)\n        else:\n            raise ValueError("Unknown synthetic function: %s" % function_name)\n\n        if noise_std > 0:\n            self.y += noise_std * torch.randn_like(self.y)\n\n    def __len__(self):\n        return self.x.size(0)\n\n    def __getitem__(self, idx):\n        return {"x": self.x[idx], "y": self.y[idx], "fidelity": self.fidelity}\n\n\n# ----------------------------------------------------------------------------- #\n#                               Loader facade                                   #\n# ----------------------------------------------------------------------------- #\n\ndef load_dataset(dataset_cfg: Dict, batch_size: int = 32) -> Tuple[Dict, int]:\n    """Load dataset according to configuration and return dataloaders per fidelity.\n\n    Returns a dict of the form {split: {fid: DataLoader}} and the input dimension.\n    """\n\n    name = dataset_cfg["name"].lower()\n    if name == "synthetic":\n        # Placeholder dataset – will be replaced with real loaders in subsequent steps\n        function_name = dataset_cfg.get("function", "branin")\n        n_low = dataset_cfg.get("n_samples_low", 200)\n        n_high = dataset_cfg.get("n_samples_high", 200)\n\n        ds_low = SyntheticFunctionDataset(n_low, fidelity=0, function_name=function_name)\n        ds_high = SyntheticFunctionDataset(n_high, fidelity=1, function_name=function_name)\n\n        # Simple 80/10/10 split per fidelity\n        def split_dataset(ds):\n            n = len(ds)\n            idxs = list(range(n))\n            random.shuffle(idxs)\n            n_train = int(0.8 * n)\n            n_val = int(0.1 * n)\n            train_idx = idxs[:n_train]\n            val_idx = idxs[n_train : n_train + n_val]\n            test_idx = idxs[n_train + n_val :]\n            return (\n                torch.utils.data.Subset(ds, train_idx),\n                torch.utils.data.Subset(ds, val_idx),\n                torch.utils.data.Subset(ds, test_idx),\n            )\n\n        splits_low = split_dataset(ds_low)\n        splits_high = split_dataset(ds_high)\n\n        dataloaders = {\n            "train": {\n                0: DataLoader(splits_low[0], batch_size=batch_size, shuffle=True),\n                1: DataLoader(splits_high[0], batch_size=batch_size, shuffle=True),\n            },\n            "val": {\n                0: DataLoader(splits_low[1], batch_size=batch_size, shuffle=False),\n                1: DataLoader(splits_high[1], batch_size=batch_size, shuffle=False),\n            },\n            "test": {\n                0: DataLoader(splits_low[2], batch_size=batch_size, shuffle=False),\n                1: DataLoader(splits_high[2], batch_size=batch_size, shuffle=False),\n            },\n        }\n        x_dim = ds_low[0]["x"].numel()\n        return dataloaders, x_dim\n\n    # --------------------------------- PLACEHOLDER --------------------------------- #\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic               #\n    # -------------------------------------------------------------------------------- #\n\n    raise NotImplementedError(f"Dataset \'{name}\' is not implemented.")', 'model_py': '"""Model architectures used across experimental variations.\n\nIncludes baseline DNN-MFBO and Reliability-Aware (RA-) DNN-MFBO. The high-fidelity\nsub-network always receives the low-fidelity prediction *mean* (not ground-truth) as\nan additional input feature. RA-DNN-MFBO learns a gating scalar to re-weight that\nfeature and includes a KL term in the loss (handled in train.py).\n"""\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# ----------------------------------------------------------------------------- #\n#                           Shared building blocks                              #\n# ----------------------------------------------------------------------------- #\n\n\nclass FidelityBlock(nn.Module):\n    def __init__(self, in_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n        # Homoscedastic noise; variance constrained positive via softplus\n        self.log_var = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        mean = self.net(x)\n        var = torch.exp(self.log_var) + 1e-6\n        return mean, var\n\n\n# ----------------------------------------------------------------------------- #\n#                            Baseline DNN-MFBO                                   #\n# ----------------------------------------------------------------------------- #\n\n\nclass DNNMFBO(nn.Module):\n    """Two-fidelity DNN-MFBO surrogate (baseline)."""\n\n    def __init__(self, x_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim, hidden_dim)\n        self.f2 = FidelityBlock(x_dim + 1, hidden_dim)\n\n    def forward_f1(self, x):\n        return self.f1(x)\n\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1, _ = self.f1(x)\n        inp = torch.cat([x, mean1], dim=-1)\n        return self.f2(inp)\n\n    def forward(self, x, fidelity: int):\n        if fidelity == 0:\n            return self.forward_f1(x)\n        else:\n            return self.forward_f2(x)\n\n\n# ----------------------------------------------------------------------------- #\n#                         Reliability-Aware DNN-MFBO                             #\n# ----------------------------------------------------------------------------- #\n\n\nclass RADNNMFBO(nn.Module):\n    """Reliability-Aware DNN-MFBO with a learnable gating scalar α ∈ (0,1)."""\n\n    def __init__(self, x_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim, hidden_dim)\n        self.alpha_raw = nn.Parameter(torch.zeros(1))  # logit-space parameter\n        self.f2 = FidelityBlock(x_dim + 1, hidden_dim)\n\n    def forward_f1(self, x):\n        return self.f1(x)\n\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1, _ = self.f1(x)\n        alpha = torch.sigmoid(self.alpha_raw)\n        inp = torch.cat([x, alpha * mean1], dim=-1)\n        return self.f2(inp)\n\n    def forward(self, x, fidelity: int):\n        if fidelity == 0:\n            return self.forward_f1(x)\n        else:\n            return self.forward_f2(x)\n\n\n# ----------------------------------------------------------------------------- #\n#                               Ablation placeholder                             #\n# ----------------------------------------------------------------------------- #\n\n\nclass ABLATION_PLACEHOLDER(nn.Module):\n    """PLACEHOLDER: Will be replaced with ablation model implementations."""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        raise NotImplementedError("This ablation model is a placeholder.")\n\n\n# ----------------------------------------------------------------------------- #\n#                               Factory function                                 #\n# ----------------------------------------------------------------------------- #\n\ndef build_model(model_name: str, x_dim: int, model_cfg: Dict | None = None):\n    model_cfg = model_cfg or {}\n    name = model_name.lower()\n    if name in {"dnn_mfbo", "baseline"}:\n        return DNNMFBO(x_dim=x_dim, **model_cfg)\n    elif name in {"radnn", "radnn_mfbo", "ra_dnn_mfbo"}:\n        return RADNNMFBO(x_dim=x_dim, **model_cfg)\n    elif name.startswith("ablation"):\n        return ABLATION_PLACEHOLDER()\n    else:\n        raise ValueError(f"Unknown model name: {model_name}")', 'main_py': 'import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport yaml\n\n# ---------------------------------------------------------------------------------- #\n#                           Helper utilities for logging                            #\n# ---------------------------------------------------------------------------------- #\n\ndef tee_stream(stream, log_file):\n    """Forward a stream to both stdout/stderr and a log file."""\n\n    def _forward():\n        for line in iter(stream.readline, ""):\n            if line:\n                log_file.write(line)\n                log_file.flush()\n                print(line, end="")\n        stream.close()\n\n    t = threading.Thread(target=_forward)\n    t.daemon = True\n    t.start()\n    return t\n\n\n# ---------------------------------------------------------------------------------- #\n#                                  Main logic                                      #\n# ---------------------------------------------------------------------------------- #\n\ndef run_experiments(config_path: Path, results_root: Path):\n    with open(config_path, "r") as f:\n        cfg = yaml.safe_load(f)\n    run_variations = cfg["run_variations"]\n\n    for run_cfg in run_variations:\n        run_id = run_cfg["run_id"]\n        print(f"\\n========== Running {run_id} ==========")\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        (run_dir / "images").mkdir(exist_ok=True)\n\n        # Save human-readable config\n        with open(run_dir / "config.yaml", "w") as f:\n            yaml.safe_dump(run_cfg, f)\n        # Save JSON config for train subprocess\n        json_cfg_path = run_dir / "config.json"\n        with open(json_cfg_path, "w") as f:\n            json.dump(run_cfg, f)\n\n        # Build subprocess command\n        cmd = [sys.executable, "-m", "src.train", "--run-config", str(json_cfg_path), "--results-dir", str(run_dir)]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n        # Tee stdout/stderr\n        stdout_log = open(run_dir / "stdout.log", "w")\n        stderr_log = open(run_dir / "stderr.log", "w")\n        threads = [\n            tee_stream(proc.stdout, stdout_log),\n            tee_stream(proc.stderr, stderr_log),\n        ]\n\n        proc.wait()\n        for t in threads:\n            t.join()\n        stdout_log.close()\n        stderr_log.close()\n\n        if proc.returncode != 0:\n            raise RuntimeError(f"Subprocess for {run_id} failed with exit code {proc.returncode}.")\n\n    # After all runs, aggregate\n    print("\\n========== Aggregating results ==========")\n    subprocess.run([sys.executable, "-m", "src.evaluate", "--results-dir", str(results_root)], check=True)\n\n\n# ---------------------------------------------------------------------------------- #\n#                                      CLI                                         #\n# ---------------------------------------------------------------------------------- #\n\n\ndef main():\n    parser = argparse.ArgumentParser(description="Master orchestrator for experiments.")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true", help="Run lightweight smoke test variations.")\n    group.add_argument("--full-experiment", action="store_true", help="Run full experiment variations.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Directory to save all results & figures.")\n    args = parser.parse_args()\n\n    root = Path(args.results_dir)\n    root.mkdir(parents=True, exist_ok=True)\n\n    if args.smoke_test:\n        config_path = Path("config") / "smoke_test.yaml"\n    else:\n        config_path = Path("config") / "full_experiment.yaml"\n\n    run_experiments(config_path, root)\n\n\nif __name__ == "__main__":\n    main()', 'pyproject_toml': '[build-system]\nrequires = ["setuptools>=61.0"]\nbuild-backend = "setuptools.build_meta"\n\n[project]\nname = "ra-dnn-mfbo-framework"\nversion = "0.1.0"\ndescription = "Common core foundation for RA-DNN-MFBO experiments"\nrequires-python = ">=3.9"\ndependencies = [\n  "torch>=2.0.0",\n  "numpy>=1.23",\n  "pyyaml>=6.0",\n  "matplotlib>=3.6",\n  "seaborn>=0.12",\n  "pandas>=1.5",\n  "scikit-learn>=1.2",\n  "tqdm>=4.65"\n]\n', 'smoke_test_yaml': '# Lightweight smoke-test configuration – executes quickly on CI\nrun_variations:\n  - run_id: baseline_smoke\n    model_name: dnn_mfbo\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    seed: 42\n\n  - run_id: radnn_smoke\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    seed: 43', 'full_experiment_yaml': '# PLACEHOLDER: Full experiment configuration – will be populated in the next phase.\n# The structure mirrors smoke_test.yaml but with real datasets, full training budgets,\n# and all baselines/ablations.\n\nrun_variations:\n  - run_id: BASELINE_PLACEHOLDER\n    model_name: MODEL_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: "..."\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 123\n\n  # Additional experiment variations will be appended here during instantiation.\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**


**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**




# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport random\nimport sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom src import model as model_lib\nfrom src import preprocess as preprocess_lib\n\n# ------------------------------- Utility functions -------------------------------- #\n\ndef set_deterministic(seed: int = 42):\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef nll_gaussian(y, mean, var):\n    \"\"\"Negative log-likelihood of isotropic Gaussian with diagonal variance.\"\"\"\n    return 0.5 * torch.log(var) + 0.5 * (y - mean) ** 2 / var\n\n\ndef nrmse(y_true: torch.Tensor, y_pred: torch.Tensor):\n    rmse = torch.sqrt(F.mse_loss(y_pred, y_true))\n    return (rmse / (y_true.max() - y_true.min())).item()\n\n\n# ------------------------------- Training routine --------------------------------- #\n\n\ndef train_single_run(run_config: dict, results_dir: Path):\n    results_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    # 1. Deterministic behaviour\n    seed = run_config.get(\"seed\", 42)\n    set_deterministic(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 2. Data\n    dataloaders, x_dim = preprocess_lib.load_dataset(run_config[\"dataset\"], batch_size=run_config[\"training\"][\"batch_size\"])\n\n    # 3. Model\n    model = model_lib.build_model(run_config[\"model_name\"], x_dim=x_dim, model_cfg=run_config.get(\"model\", {})).to(device)\n\n    # 4. Optimiser & misc\n    lr = run_config[\"training\"].get(\"lr\", 1e-3)\n    epochs = run_config[\"training\"].get(\"epochs\", 100)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    train_loss_hist, val_loss_hist, val_nrmse_hist = [], [], []\n    best_val_nrmse = float(\"inf\")\n\n    for epoch in tqdm(range(1, epochs + 1), desc=f\"{run_config['run_id']}\"):\n        model.train()\n        epoch_losses = []\n\n        # --- iterate low then high fidelity to guarantee low-forward availability ---\n        for fidelity in [0, 1]:\n            for batch in dataloaders[\"train\"][fidelity]:\n                x = batch[\"x\"].to(device)\n                y = batch[\"y\"].to(device)\n\n                if fidelity == 0:\n                    mean, var = model.forward_f1(x)\n                    loss = nll_gaussian(y, mean, var).mean()\n                else:  # high fidelity\n                    mean, var = model.forward_f2(x)\n                    loss = nll_gaussian(y, mean, var).mean()\n                    # Add KL on alpha if present (RADNN)\n                    if hasattr(model, \"alpha_raw\"):\n                        alpha = torch.sigmoid(model.alpha_raw)\n                        kl_alpha = -torch.distributions.Beta(1.0, 1.0).log_prob(alpha)\n                        loss = loss + run_config.get(\"kl_coeff\", 0.01) * kl_alpha\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                epoch_losses.append(loss.item())\n\n        train_loss_hist.append(np.mean(epoch_losses))\n\n        # ---------------- Validation ----------------\n        model.eval()\n        with torch.no_grad():\n            val_losses, preds, targets = [], [], []\n            for batch in dataloaders[\"val\"][1]:  # high fidelity only for val metrics\n                x = batch[\"x\"].to(device)\n                y = batch[\"y\"].to(device)\n                mean, var = model.forward_f2(x)\n                loss = nll_gaussian(y, mean, var).mean()\n                val_losses.append(loss.item())\n                preds.append(mean.cpu())\n                targets.append(y.cpu())\n        val_loss = np.mean(val_losses)\n        val_loss_hist.append(val_loss)\n\n        preds = torch.cat(preds, dim=0)\n        targets = torch.cat(targets, dim=0)\n        val_nrmse_val = nrmse(targets, preds)\n        val_nrmse_hist.append(val_nrmse_val)\n        best_val_nrmse = min(best_val_nrmse, val_nrmse_val)\n\n    # ---------------- Testing ----------------\n    model.eval()\n    with torch.no_grad():\n        preds, targets = [], []\n        for batch in dataloaders[\"test\"][1]:\n            x, y = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            mean, _ = model.forward_f2(x)\n            preds.append(mean.cpu())\n            targets.append(y.cpu())\n    preds = torch.cat(preds, dim=0)\n    targets = torch.cat(targets, dim=0)\n    test_nrmse_val = nrmse(targets, preds)\n\n    # ---------------- Save artefacts ----------------\n    torch.save(model.state_dict(), results_dir / \"model.pt\")\n\n    metrics = {\n        \"run_id\": run_config[\"run_id\"],\n        \"best_val_nrmse\": best_val_nrmse,\n        \"test_nrmse\": test_nrmse_val,\n        \"train_loss_hist\": train_loss_hist,\n        \"val_loss_hist\": val_loss_hist,\n        \"val_nrmse_hist\": val_nrmse_hist,\n        \"config\": run_config,\n    }\n\n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # ------------------ Figures --------------------\n    sns.set(style=\"whitegrid\", font_scale=1.3)\n\n    # 1. Training / validation loss curves\n    plt.figure(figsize=(6, 4))\n    plt.plot(train_loss_hist, label=\"Train NLL\")\n    plt.plot(val_loss_hist, label=\"Val NLL\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Negative Log-Likelihood\")\n    plt.title(f\"Training Curve – {run_config['run_id']}\")\n    plt.legend()\n    plt.annotate(f\"Final Val = {val_loss_hist[-1]:.3f}\", xy=(len(val_loss_hist) - 1, val_loss_hist[-1]))\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"training_loss_{run_config['run_id']}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # 2. nRMSE curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(val_nrmse_hist, label=\"Val nRMSE\", color=\"green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"nRMSE\")\n    plt.title(f\"Validation nRMSE – {run_config['run_id']}\")\n    plt.annotate(f\"Best = {best_val_nrmse:.3f}\", xy=(np.argmin(val_nrmse_hist), best_val_nrmse))\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(images_dir / f\"nrmse_{run_config['run_id']}.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # Print JSON summary to stdout for structured logging\n    print(json.dumps({\"run_id\": run_config[\"run_id\"], \"best_val_nrmse\": best_val_nrmse, \"test_nrmse\": test_nrmse_val}))\n\n\n# ---------------------------------------------------------------------------------- #\n\n\ndef cli_main():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to a JSON file containing the run configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to write outputs to.\")\n    args = parser.parse_args()\n\n    with open(args.run_config, \"r\") as f:\n        run_cfg = json.load(f)\n\n    train_single_run(run_cfg, Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    cli_main()",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# ------------------------------- Evaluation/visualisation ------------------------- #\n\n\ndef collect_results(results_root: Path):\n    summaries = []\n    for run_dir in results_root.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as f:\n                data = json.load(f)\n                summaries.append({\n                    \"run_id\": data[\"run_id\"],\n                    \"best_val_nrmse\": data[\"best_val_nrmse\"],\n                    \"test_nrmse\": data[\"test_nrmse\"],\n                })\n    return pd.DataFrame(summaries)\n\n\ndef generate_comparison_figures(df: pd.DataFrame, results_dir: Path):\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    sns.set(style=\"whitegrid\", font_scale=1.4)\n\n    # 1. Bar chart of test nRMSE\n    plt.figure(figsize=(8, 5))\n    ax = sns.barplot(x=\"run_id\", y=\"test_nrmse\", data=df, palette=\"viridis\")\n    for i, p in enumerate(ax.patches):\n        val = df.loc[i, \"test_nrmse\"]\n        ax.annotate(f\"{val:.3f}\", (p.get_x() + p.get_width() / 2., val), ha='center', va='bottom')\n    plt.ylabel(\"Test nRMSE (lower is better)\")\n    plt.title(\"Test nRMSE comparison across runs\")\n    plt.tight_layout()\n    plt.savefig(images_dir / \"test_nrmse_comparison.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # 2. Sorted line plot for clarity (optional)\n    df_sorted = df.sort_values(\"test_nrmse\")\n    plt.figure(figsize=(8, 5))\n    plt.plot(df_sorted[\"run_id\"], df_sorted[\"test_nrmse\"], marker=\"o\")\n    for i, val in enumerate(df_sorted[\"test_nrmse\"].values):\n        plt.annotate(f\"{val:.3f}\", (i, val))\n    plt.xlabel(\"Run\")\n    plt.ylabel(\"Test nRMSE\")\n    plt.title(\"Test nRMSE per run (sorted)\")\n    plt.tight_layout()\n    plt.savefig(images_dir / \"test_nrmse_sorted.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\n# ------------------------------ Main entry-point ---------------------------------- #\n\n\ndef main(results_root: str):\n    root = Path(results_root)\n    df = collect_results(root)\n    generate_comparison_figures(df, root)\n\n    # Print summary JSON to stdout\n    print(df.to_json(orient=\"records\"))\n\n\n# ---------------------------------------------------------------------------------- #\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Aggregate and visualise results across runs.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    main(args.results_dir)",
    "preprocess_py": "\"\"\"Dataset loading and common preprocessing utilities.\n\nAll experimental variations *must* rely on this file for data access so that preprocessing\nremains identical across runs. Implements simple synthetic two–fidelity benchmarks that\nare inexpensive yet sufficiently rich to validate the complete experimental pipeline.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# ----------------------------------------------------------------------------- #\n#                               Synthetic functions                              #\n# ----------------------------------------------------------------------------- #\n\n\ndef _branin(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"High-fidelity Branin function (2-D).\"\"\"\n    x1, x2 = x[..., 0], x[..., 1]\n    a = 1.0\n    b = 5.1 / (4 * math.pi ** 2)\n    c = 5 / math.pi\n    r = 6.0\n    s = 10.0\n    t = 1.0 / (8.0 * math.pi)\n    return a * (x2 - b * x1 ** 2 + c * x1 - r) ** 2 + s * (1 - t) * torch.cos(x1) + s\n\n\ndef _low_fidelity_branin(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Ad-hoc cheaper Branin variant – smooths and adds bias/artefacts.\"\"\"\n    return 0.5 * _branin(x) + 10.0 * torch.sin(x[..., 0])\n\n\ndef _levy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"High-fidelity 2-D Levy function.\"\"\"\n    # Standard Levy implementation adapted for two dimensions\n    w = 1 + (x - 1) / 4.0  # shape (..., 2)\n    term1 = torch.sin(math.pi * w[..., 0]) ** 2\n    term3 = (w[..., 1] - 1) ** 2 * (1 + torch.sin(2 * math.pi * w[..., 1]) ** 2)\n    term2 = (w[..., 0] - 1) ** 2 * (\n        1 + 10 * torch.sin(math.pi * w[..., 0] + 1) ** 2\n    )\n    return term1 + term2 + term3\n\n\ndef _low_fidelity_levy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Low-fidelity Levy – down-scaled + periodic bias.\"\"\"\n    return 0.4 * _levy(x) + 5.0 * torch.cos(0.5 * x[..., 0])\n\n\n# Mapping for convenience ------------------------------------------------------- #\n_SYNTHETIC_FUNCS = {\n    \"branin\": (_low_fidelity_branin, _branin, (-5.0, 10.0, 0.0, 15.0)),\n    \"levy\": (_low_fidelity_levy, _levy, (-10.0, 10.0, -10.0, 10.0)),\n}\n\n\n# ----------------------------------------------------------------------------- #\n#                           Synthetic dataset wrapper                            #\n# ----------------------------------------------------------------------------- #\n\n\nclass SyntheticFunctionDataset(Dataset):\n    \"\"\"Generates random (x, y) pairs on-the-fly for a given fidelity.\"\"\"\n\n    def __init__(\n        self,\n        n_samples: int,\n        fidelity: int,\n        function_name: str = \"branin\",\n        noise_std: float = 0.0,\n    ):\n        super().__init__()\n        if function_name.lower() not in _SYNTHETIC_FUNCS:\n            raise ValueError(f\"Unsupported synthetic function: {function_name}\")\n\n        low_f, high_f, domain = _SYNTHETIC_FUNCS[function_name.lower()]\n        self.fidelity = fidelity  # 0 = low, 1 = high\n        self.noise_std = noise_std\n\n        # Sample uniformly in the function-specific domain\n        x1_min, x1_max, x2_min, x2_max = domain\n        x1 = torch.FloatTensor(n_samples).uniform_(x1_min, x1_max)\n        x2 = torch.FloatTensor(n_samples).uniform_(x2_min, x2_max)\n        self.x = torch.stack([x1, x2], dim=-1)\n\n        if fidelity == 0:\n            self.y = low_f(self.x)\n        else:\n            self.y = high_f(self.x)\n\n        if noise_std > 0:\n            self.y += noise_std * torch.randn_like(self.y)\n\n        # Normalise outputs to [0, 1] for numerical stability ------------------ #\n        y_min, y_max = self.y.min(), self.y.max()\n        self.y = (self.y - y_min) / (y_max - y_min + 1e-8)\n\n        # Scale inputs to [0, 1] as well for both axes ------------------------- #\n        self.x[..., 0] = (self.x[..., 0] - x1_min) / (x1_max - x1_min)\n        self.x[..., 1] = (self.x[..., 1] - x2_min) / (x2_max - x2_min)\n\n    # ------------------------------ PyTorch API ---------------------------------- #\n\n    def __len__(self):\n        return self.x.size(0)\n\n    def __getitem__(self, idx):\n        return {\n            \"x\": self.x[idx],\n            \"y\": self.y[idx],\n            \"fidelity\": torch.tensor(self.fidelity, dtype=torch.long),\n        }\n\n\n# ----------------------------------------------------------------------------- #\n#                               Public loader                                   #\n# ----------------------------------------------------------------------------- #\n\ndef load_dataset(dataset_cfg: Dict, batch_size: int = 32) -> Tuple[Dict, int]:\n    \"\"\"Create train/val/test DataLoaders per fidelity.\n\n    The returned dictionary has structure:\n        {\n            \"train\": {0: DataLoader, 1: DataLoader},\n            \"val\":   {0: DataLoader, 1: DataLoader},\n            \"test\":  {0: DataLoader, 1: DataLoader},\n        }\n    and the second return value is the input dimensionality (always 2 here).\n    \"\"\"\n\n    name = dataset_cfg[\"name\"].lower()\n\n    if name == \"synthetic\":\n        function_name = dataset_cfg.get(\"function\", \"branin\")\n        n_low = int(dataset_cfg.get(\"n_samples_low\", 500))\n        n_high = int(dataset_cfg.get(\"n_samples_high\", 500))\n        noise_std = float(dataset_cfg.get(\"noise_std\", 0.0))\n\n        ds_low = SyntheticFunctionDataset(\n            n_low, fidelity=0, function_name=function_name, noise_std=noise_std\n        )\n        ds_high = SyntheticFunctionDataset(\n            n_high, fidelity=1, function_name=function_name, noise_std=noise_std\n        )\n\n        # 80 / 10 / 10 random split per fidelity -------------------------------- #\n        def _split(ds):\n            n = len(ds)\n            idxs = list(range(n))\n            random.shuffle(idxs)\n            n_train = int(0.8 * n)\n            n_val = int(0.1 * n)\n            train_idx = idxs[:n_train]\n            val_idx = idxs[n_train : n_train + n_val]\n            test_idx = idxs[n_train + n_val :]\n            return (\n                torch.utils.data.Subset(ds, train_idx),\n                torch.utils.data.Subset(ds, val_idx),\n                torch.utils.data.Subset(ds, test_idx),\n            )\n\n        splits_low = _split(ds_low)\n        splits_high = _split(ds_high)\n\n        dataloaders = {\n            \"train\": {\n                0: DataLoader(splits_low[0], batch_size=batch_size, shuffle=True),\n                1: DataLoader(splits_high[0], batch_size=batch_size, shuffle=True),\n            },\n            \"val\": {\n                0: DataLoader(splits_low[1], batch_size=batch_size, shuffle=False),\n                1: DataLoader(splits_high[1], batch_size=batch_size, shuffle=False),\n            },\n            \"test\": {\n                0: DataLoader(splits_low[2], batch_size=batch_size, shuffle=False),\n                1: DataLoader(splits_high[2], batch_size=batch_size, shuffle=False),\n            },\n        }\n        x_dim = ds_low[0][\"x\"].numel()\n        return dataloaders, x_dim\n\n    # ------------------------------------------------------------------------- #\n    # If we ever reach here the user requested an unsupported dataset ---------- #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented in preprocess.py.\")",
    "model_py": "\"\"\"Model architectures used across experimental variations.\n\nThis file contains fully-implemented surrogates for all experiment variants:\n    • DNN-MFBO baseline (hard-wired low-fidelity mean as extra feature)\n    • RA-DNN-MFBO (learnable gate with KL regularisation)\n    • RA-DNN-MFBO-noKL (same architecture, KL coefficient will be 0 in the config)\n    • RA-DNN-MFBO-fixed-alpha1 (degenerates to baseline)\n    • RA-DNN-MFBO-fixed-alpha0 (ignores low-fidelity feature altogether)\nEvery model exposes .forward_f1 and .forward_f2 for low & high fidelities so that the\ntraining loop in src/train.py remains identical across variants.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n# ----------------------------------------------------------------------------- #\n#                             Shared building block                              #\n# ----------------------------------------------------------------------------- #\n\n\nclass FidelityBlock(nn.Module):\n    \"\"\"Simple 2-layer MLP producing a Gaussian mean & homoscedastic variance.\"\"\"\n\n    def __init__(self, in_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n        self.log_var = nn.Parameter(torch.zeros(1))  # log σ² – unconstrained\n\n    def forward(self, x):\n        mean = self.net(x)\n        var = torch.exp(self.log_var) + 1e-6  # numeric stability\n        return mean, var\n\n\n# ----------------------------------------------------------------------------- #\n#                          Baseline: DNN-MFBO surrogate                          #\n# ----------------------------------------------------------------------------- #\n\n\nclass DNNMFBO(nn.Module):\n    \"\"\"Original stacked surrogate from the DNN-MFBO paper (two fidelities).\"\"\"\n\n    def __init__(self, x_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim, hidden_dim)\n        self.f2 = FidelityBlock(x_dim + 1, hidden_dim)\n\n    # ------------------------- Low-fidelity ----------------------------------- #\n    def forward_f1(self, x):\n        return self.f1(x)\n\n    # ------------------------ High-fidelity ----------------------------------- #\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1, _ = self.f1(x)\n        inp = torch.cat([x, mean1], dim=-1)\n        return self.f2(inp)\n\n    def forward(self, x, fidelity: int):\n        return self.forward_f1(x) if fidelity == 0 else self.forward_f2(x)\n\n\n# ----------------------------------------------------------------------------- #\n#                     Reliability-Aware DNN-MFBO surrogate                       #\n# ----------------------------------------------------------------------------- #\n\n\nclass RADNNMFBO(nn.Module):\n    \"\"\"RA-DNN-MFBO with learnable gate α ∈ (0, 1).\"\"\"\n\n    def __init__(self, x_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim, hidden_dim)\n        # α parameterised in logit space so unconstrained optimisation in ℝ\n        self.alpha_raw = nn.Parameter(torch.zeros(1))\n        self.f2 = FidelityBlock(x_dim + 1, hidden_dim)\n\n    def forward_f1(self, x):\n        return self.f1(x)\n\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1, _ = self.f1(x)\n        alpha = torch.sigmoid(self.alpha_raw)\n        inp = torch.cat([x, alpha * mean1], dim=-1)\n        return self.f2(inp)\n\n    def forward(self, x, fidelity: int):\n        return self.forward_f1(x) if fidelity == 0 else self.forward_f2(x)\n\n\n# ----------------------------------------------------------------------------- #\n#                          Fixed-α ablation models                               #\n# ----------------------------------------------------------------------------- #\n\n\nclass FixedAlphaOneDNNMFBO(DNNMFBO):\n    \"\"\"Uses α = 1 – identical to the baseline but kept for clarity.\"\"\"\n\n    # Inherits everything from baseline; no modifications necessary.\n    pass\n\n\nclass FixedAlphaZeroDNNMFBO(nn.Module):\n    \"\"\"α = 0 → ignore low-fidelity mean for the high-fidelity network.\"\"\"\n\n    def __init__(self, x_dim: int, hidden_dim: int = 50):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim, hidden_dim)\n        # Note: f2 takes *only* x (no extra feature)\n        self.f2 = FidelityBlock(x_dim, hidden_dim)\n\n    def forward_f1(self, x):\n        return self.f1(x)\n\n    def forward_f2(self, x):\n        # Still compute mean1 so computational graph / call signature identical\n        # but deliberately *ignore* it when forming inputs to f2.\n        _mean1, _ = self.f1(x)  # noqa: F841 – value intentionally unused\n        return self.f2(x)\n\n    def forward(self, x, fidelity: int):\n        return self.forward_f1(x) if fidelity == 0 else self.forward_f2(x)\n\n\n# ----------------------------------------------------------------------------- #\n#                               Factory function                                 #\n# ----------------------------------------------------------------------------- #\n\ndef build_model(model_name: str, x_dim: int, model_cfg: Dict | None = None):\n    \"\"\"Instantiate a model by name so YAML stays human-friendly.\"\"\"\n\n    model_cfg = model_cfg or {}\n    name = model_name.lower()\n\n    if name in {\"dnn_mfbo\", \"baseline\"}:\n        return DNNMFBO(x_dim=x_dim, **model_cfg)\n\n    if name in {\"radnn\", \"radnn_mfbo\", \"ra_dnn_mfbo\"}:\n        return RADNNMFBO(x_dim=x_dim, **model_cfg)\n\n    if name in {\"radnn_fixed_alpha1\", \"fixed_alpha1\"}:\n        return FixedAlphaOneDNNMFBO(x_dim=x_dim, **model_cfg)\n\n    if name in {\"radnn_fixed_alpha0\", \"fixed_alpha0\"}:\n        return FixedAlphaZeroDNNMFBO(x_dim=x_dim, **model_cfg)\n\n    raise ValueError(f\"Unknown model name: {model_name}\")",
    "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\n\nimport yaml\n\n# ---------------------------------------------------------------------------------- #\n#                           Helper utilities for logging                            #\n# ---------------------------------------------------------------------------------- #\n\ndef tee_stream(stream, log_file):\n    \"\"\"Forward a stream to both stdout/stderr and a log file.\"\"\"\n\n    def _forward():\n        for line in iter(stream.readline, \"\"):\n            if line:\n                log_file.write(line)\n                log_file.flush()\n                print(line, end=\"\")\n        stream.close()\n\n    t = threading.Thread(target=_forward)\n    t.daemon = True\n    t.start()\n    return t\n\n\n# ---------------------------------------------------------------------------------- #\n#                                  Main logic                                      #\n# ---------------------------------------------------------------------------------- #\n\ndef run_experiments(config_path: Path, results_root: Path):\n    with open(config_path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    run_variations = cfg[\"run_variations\"]\n\n    for run_cfg in run_variations:\n        run_id = run_cfg[\"run_id\"]\n        print(f\"\\n========== Running {run_id} ==========\")\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        (run_dir / \"images\").mkdir(exist_ok=True)\n\n        # Save human-readable config\n        with open(run_dir / \"config.yaml\", \"w\") as f:\n            yaml.safe_dump(run_cfg, f)\n        # Save JSON config for train subprocess\n        json_cfg_path = run_dir / \"config.json\"\n        with open(json_cfg_path, \"w\") as f:\n            json.dump(run_cfg, f)\n\n        # Build subprocess command\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--run-config\", str(json_cfg_path), \"--results-dir\", str(run_dir)]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n        # Tee stdout/stderr\n        stdout_log = open(run_dir / \"stdout.log\", \"w\")\n        stderr_log = open(run_dir / \"stderr.log\", \"w\")\n        threads = [\n            tee_stream(proc.stdout, stdout_log),\n            tee_stream(proc.stderr, stderr_log),\n        ]\n\n        proc.wait()\n        for t in threads:\n            t.join()\n        stdout_log.close()\n        stderr_log.close()\n\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess for {run_id} failed with exit code {proc.returncode}.\")\n\n    # After all runs, aggregate\n    print(\"\\n========== Aggregating results ==========\")\n    subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)], check=True)\n\n\n# ---------------------------------------------------------------------------------- #\n#                                      CLI                                         #\n# ---------------------------------------------------------------------------------- #\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Master orchestrator for experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run lightweight smoke test variations.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to save all results & figures.\")\n    args = parser.parse_args()\n\n    root = Path(args.results_dir)\n    root.mkdir(parents=True, exist_ok=True)\n\n    if args.smoke_test:\n        config_path = Path(\"config\") / \"smoke_test.yaml\"\n    else:\n        config_path = Path(\"config\") / \"full_experiment.yaml\"\n\n    run_experiments(config_path, root)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ra-dnn-mfbo-framework\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for RA-DNN-MFBO experiments\"\nrequires-python = \">=3.9\"\ndependencies = [\n  \"torch>=2.0.0\",\n  \"numpy>=1.23\",\n  \"pyyaml>=6.0\",\n  \"matplotlib>=3.6\",\n  \"seaborn>=0.12\",\n  \"pandas>=1.5\",\n  \"scikit-learn>=1.2\",\n  \"tqdm>=4.65\"\n]\n",
    "smoke_test_yaml": "# Lightweight smoke-test configuration – executes within seconds on CI runners\nrun_variations:\n  - run_id: baseline_smoke\n    model_name: dnn_mfbo\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    seed: 42\n\n  - run_id: radnn_smoke\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    kl_coeff: 0.01\n    seed: 43\n\n  - run_id: radnn_noKL_smoke\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    kl_coeff: 0.0  # disables KL term\n    seed: 44\n\n  - run_id: radnn_fixed_alpha1_smoke\n    model_name: radnn_fixed_alpha1\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    seed: 45\n\n  - run_id: radnn_fixed_alpha0_smoke\n    model_name: radnn_fixed_alpha0\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 100\n      n_samples_high: 100\n    training:\n      epochs: 5\n      batch_size: 32\n      lr: 1e-3\n    seed: 46",
    "full_experiment_yaml": "# Full experimental configuration for exp-main-perf\n# Covers both Branin & Levy synthetic benchmarks and all five model variants.\n\nrun_variations:\n  # ---------------------- Branin runs ---------------------------------------- #\n  - run_id: baseline-dnn-mfbo-branin\n    model_name: dnn_mfbo\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 101\n\n  - run_id: ra-dnn-mfbo-branin\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    kl_coeff: 0.01\n    seed: 102\n\n  - run_id: ra-dnn-mfbo-noKL-branin\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    kl_coeff: 0.0\n    seed: 103\n\n  - run_id: ra-dnn-fixed-alpha1-branin\n    model_name: radnn_fixed_alpha1\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 104\n\n  - run_id: ra-dnn-fixed-alpha0-branin\n    model_name: radnn_fixed_alpha0\n    dataset:\n      name: synthetic\n      function: branin\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 105\n\n  # ----------------------- Levy runs ----------------------------------------- #\n  - run_id: baseline-dnn-mfbo-levy\n    model_name: dnn_mfbo\n    dataset:\n      name: synthetic\n      function: levy\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 201\n\n  - run_id: ra-dnn-mfbo-levy\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: levy\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    kl_coeff: 0.01\n    seed: 202\n\n  - run_id: ra-dnn-mfbo-noKL-levy\n    model_name: radnn\n    dataset:\n      name: synthetic\n      function: levy\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    kl_coeff: 0.0\n    seed: 203\n\n  - run_id: ra-dnn-fixed-alpha1-levy\n    model_name: radnn_fixed_alpha1\n    dataset:\n      name: synthetic\n      function: levy\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 204\n\n  - run_id: ra-dnn-fixed-alpha0-levy\n    model_name: radnn_fixed_alpha0\n    dataset:\n      name: synthetic\n      function: levy\n      n_samples_low: 800\n      n_samples_high: 800\n    training:\n      epochs: 100\n      batch_size: 64\n      lr: 1e-3\n    seed: 205"
}
