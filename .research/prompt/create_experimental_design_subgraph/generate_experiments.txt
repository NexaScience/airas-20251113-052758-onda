
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 2 distinct Experiment objects.

# Instructions
- Generate 2 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 2 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "DNN-MFBO hard-wires the previous-fidelity output fm-1(x) as an unscaled input feature for fm(x).\nIf the correlation between fidelities is weak or changes locally, this unconditional use of fm-1(x) can introduce negative transfer, slowing hyper-parameter search and occasionally selecting misleading low-cost queries. A minimal mechanism is needed to adaptively reduce or amplify the influence of lower fidelities without redesigning the architecture or acquisition.",
    "Methods": "Reliability-Aware DNN-MFBO (RA-DNN-MFBO)\nMinimal change: insert one learnable scalar gate αm∈[0,1] for every fidelity m>1.\nInput becomes   xm=[x ; αm·fm-1(x)]   instead of   [x ; fm-1(x)].\nPrior: αm∼Beta(1,1)  (uniform). In variational inference we keep a Gaussian variational posterior on logit(αm). An extra KL( q(αm) || p(αm) ) term is added to the ELBO (two additional lines of code).\nMotivation:\n• If fm-1 is unreliable, the posterior pushes αm→0, avoiding negative transfer.\n• When fidelities are strongly correlated the network learns αm→1, recovering the original model.\n• This single multiplicative gate is differentiable, cheap, and does not change acquisition-function derivations (moment matching still applies because scaling a Gaussian keeps it Gaussian).",
    "Experimental Setup": "Data: 2-fidelity Branin and Levy benchmarks supplied with DNN-MFBO code (low-fidelity = cheap interpolation, high-fidelity = ground truth).\nBaselines: Original DNN-MFBO vs proposed RA-DNN-MFBO.\nInitial design: 10 random queries per fidelity.\nBudget: cost-weighted budget of 150 (low-fidelity cost 1, high-fidelity cost 10).\nMetrics: simple regret w.r.t. cost, plus nRMSE of surrogate on 100 test points to isolate modelling effect.\nRepeats: 10 random seeds.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass FidelityBlock(nn.Module):\n    def __init__(self, in_dim, hid=50):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(in_dim, hid), nn.ReLU(),\n                                 nn.Linear(hid, 1))\n        self.log_var = nn.Parameter(torch.zeros(1))  # homoscedastic noise\n    def forward(self, x):\n        mean = self.net(x)\n        var  = torch.exp(self.log_var)\n        return mean, var\n\nclass RADNNMFBO(nn.Module):\n    \"\"\"2-fidelity version for demo\"\"\"\n    def __init__(self, x_dim):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim)\n        self.alpha_raw = nn.Parameter(torch.zeros(1))  # sigmoid→α∈(0,1)\n        self.f2 = FidelityBlock(x_dim+1)\n    def forward_f1(self, x):\n        return self.f1(x)\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1,_ = self.f1(x)\n        alpha = torch.sigmoid(self.alpha_raw)\n        inp = torch.cat([x, alpha*mean1], dim=-1)\n        return self.f2(inp)\n# --- tiny training loop for surrogate only ---\nopt = torch.optim.Adam(model.parameters(), 1e-3)\nfor itr in range(1000):\n    m1,v1 = model.forward_f1(x1);  m2,v2 = model.forward_f2(x2)\n    nll  = ((y1-m1)**2/v1 + torch.log(v1)).mean()\n    nll += ((y2-m2)**2/v2 + torch.log(v2)).mean()\n    # Beta(1,1) prior KL on α (Gaussian approx):\n    kl_alpha = -torch.distributions.Beta(1,1).log_prob(torch.sigmoid(model.alpha_raw))\n    loss = nll + 0.01*kl_alpha\n    opt.zero_grad(); loss.backward(); opt.step()",
    "Expected Result": "Across both test functions RA-DNN-MFBO attains ~5-10% lower simple regret after the same cost budget and 15-20% lower nRMSE of the high-fidelity surrogate.\nGating coefficients converge towards α≈0.3 on Levy (weak correlation) and α≈0.9 on Branin (strong correlation), confirming adaptive behaviour. Runtime overhead is negligible (<2%).",
    "Expected Conclusion": "Introducing a single learnable gate per fidelity is enough to prevent negative transfer when fidelities are weakly correlated, while leaving the original behaviour intact when correlations are strong. The change is architecturally trivial, requires only one extra parameter and a tiny KL term, yet yields consistent efficiency gains in hyper-parameter optimization. Such reliability-aware gating can be plugged into any stacked multi-fidelity neural surrogate with virtually no additional computational cost."
}

# Experiment Strategy
Goal:
Establish that Reliability-Aware DNN-MFBO (RA-DNN-MFBO) is a generally better drop-in replacement for DNN-MFBO and other multi-fidelity BO surrogates in terms of (1) search performance, (2) modelling fidelity, (3) robustness to fidelity-correlation changes, (4) computational efficiency, and (5) interpretability of the learnt gate. Every individual experiment will be an instantiation of the procedure outlined below.

1. Hypotheses to Validate
1.1 Performance: RA-DNN-MFBO reaches lower cost-weighted simple regret than baselines under the same budget.
1.2 Modelling Accuracy: The high-fidelity predictive nRMSE of RA-DNN-MFBO is lower, showing that the gate improves the surrogate itself.
1.3 Robustness: When low/high fidelity correlation is weak, adversarially non-stationary, noisy, or task-dependent, RA-DNN-MFBO avoids negative transfer while baselines suffer.
1.4 Efficiency: Added wall-clock time, GPU memory and number of parameters stay within +5% of original DNN-MFBO.
1.5 Generalization: Gains persist across (a) synthetic benchmarks, (b) public black-box optimisation suites, and (c) at least one applied industrial task.
1.6 Interpretability: Learnt αm values correlate with empirical inter-fidelity correlation coefficients; values shrink where correlation is low.

2. Comparative Landscape
2.1 Core Baselines
  • Original DNN-MFBO (hard-wired fm-1).  
  • State-of-the-art GP-based MFBO (e.g. MF-GP-EI) to show competitiveness beyond the DNN family.  
2.2 Ablations (inside RA-DNN-MFBO)  
  • Fixed αm=1 (identical to baseline)  
  • Fixed αm=0 (no low-fidelity input)  
  • Learnable αm but no KL-regularisation  
  • Per-dimension gates vs single scalar gate (complexity control).
2.3 Drop-in Test: Replace the surrogate in an existing BO pipeline (e.g. Bayesian optimisation library) with RA-DNN-MFBO and compare end-to-end quality.

3. Experimental Angles & Metrics
3.1 Quantitative
  • Cost-weighted simple regret curve (primary).  
  • Area-under-curve (AUC) of regret.  
  • nRMSE / NLL on held-out high-fidelity points.  
  • Wall-clock time, FLOPs and peak GPU memory.  
  • Variance across ≥10 random seeds; report mean±95% CI.
3.2 Qualitative / Diagnostic
  • Evolution of αm during optimisation vs estimated fidelity-correlation.  
  • Visualisations of posterior mean & variance slices to show reduced pathological extrapolation.  
  • Acquisition heat-maps showing fewer misleading low-cost queries under weak correlation.
3.3 Stress/Robustness
  • Correlation sweep: For synthetic tasks continuously morph the low-fidelity function; measure performance break-points.  
  • Noise injection: Add controlled Gaussian noise to fm-1(x).  
  • Fidelity shift: Mid-run swap low-fidelity function; examine recovery speed.

4. Unified Protocol per Experiment
Step-A   Select task/problem (with known cost model).  
Step-B   Specify budgets (cost or wall-clock) and identical initial design.  
Step-C   Run RA-DNN-MFBO + all baselines/ablations for N_seeds seeds.  
Step-D   Log metrics in real time; snapshot models every K BO steps for later analysis.  
Step-E   Post-process: compute statistics, significance tests (paired t-test/Wilcoxon, Holm correction).  
Step-F   Generate plots/tables; archive raw logs for reproducibility.

5. Environment & Reproducibility Controls
  • Hardware: Single NVIDIA A100-80GB, 2048 GB RAM node; enforce identical PyTorch/cuDNN versions.
  • Deterministic seeds, torch.backends.cudnn.deterministic=True.  
  • Automatic mixed precision OFF to keep numerical paths identical.  
  • Use WandB/MLFlow for central logging; store config & git commit hash.  
  • Provide container (Docker/Singularity) with all dependencies.

6. Success Criteria
  • At least 5% lower final simple regret AND ≥10% better AUC on ≥70% of benchmark tasks, statistically significant (p<0.05).  
  • nRMSE improvement ≥10% on ≥70% tasks.  
  • Runtime/memory overhead ≤5%.  
  • Under correlation-sweep experiments, RA curve stays within 10% of optimal (oracle choosing α per task) while baseline degrades ≥20%.  
  • αm values exhibit Pearson r>0.6 with empirical correlation estimates.

7. Multi-Perspective Demonstration
  • Algorithmic effectiveness: regret & accuracy.  
  • Practical efficiency: overhead metrics.  
  • Adaptivity & interpretability: gate behaviour.  
  • Broad applicability: synthetic → standard BO benchmarks → real application.  
  • Robustness: stress tests.

This unified experimental strategy ensures every subsequent experiment consistently interrogates the same core claims, produces comparable evidence across domains, and collectively validates RA-DNN-MFBO as a minimal yet powerful upgrade over existing multi-fidelity surrogates.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-main-perf",
            "run_variations": [
                "baseline-dnn-mfbo",
                "ra-dnn-mfbo",
                "ra-dnn-mfbo-noKL",
                "ra-dnn-fixed-alpha1",
                "ra-dnn-fixed-alpha0"
            ],
            "description": "Objective / Hypothesis:\nValidate H1 (performance) and H2 (modelling accuracy) while checking H4 (efficiency).  The proposed Reliability-Aware gate should (1) lower cost-weighted simple regret and (2) reduce nRMSE of the high-fidelity surrogate with ≤5 % compute overhead.\n\nBenchmarks & Datasets:\n• Synthetic 2-fidelity Branin & Levy (exact definition in DNN-MFBO repo)  \n• Black-box-optimisation suite BBOB-MF (Surrogates 101, 12 functions × 2 fidelities)  \n• NAS-Bench-201-MF (epoch-truncated CIFAR-10 training costs: 4, 12, 200 epochs → collapse to two fidelities: 4 vs 200)  \n\nPre-processing:\n• Normalise continuous input dimensions to [0,1]  \n• Min-max normalise low- & high-fidelity outputs separately  \n• Log-transform cost values, then scale to mean 0, std 1 before passing to acquisition  \n\nData Splitting:\nBO automatically generates data; hold-out test sets of 100 Latin-Hypercube points per function for surrogate nRMSE/NLL.  \n\nNumber of repetitions:\n10 seeds per benchmark. Report mean±95 % CI. Surrogate snapshot every 5 BO steps.  \n\nEvaluation Metrics:\nPrimary: cost-weighted simple regret @ budget 150 (synthetic) / 3 × true-high-fidelity cost (BBOB-MF) / 8 GPU-hours (NAS).  Secondary: AUC of regret curve, nRMSE, NLL, wall-clock time, peak GPU memory, FLOPs (fvcore), #parameters.  \n\nComparisons inside experiment:\n1. baseline-dnn-mfbo – original hard-wired model  \n2. ra-dnn-mfbo – proposed learnt gate + KL  \n3. ra-dnn-mfbo-noKL – drop KL term to test regularisation importance  \n4. ra-dnn-fixed-alpha1 – force α=1 (identical to baseline feature-wise)  \n5. ra-dnn-fixed-alpha0 – remove low-fidelity feature entirely  \nExternal reference (plotted but not in run_variations): MF-GP-EI from BoTorch, using default hyper-priors.  \n\nHyper-parameter Grid (analysed post-hoc): learning-rate ∈{1e-4,3e-4,1e-3}; network width {32,64}; KL-weight β∈{0.005,0.01,0.05}. Swept on Branin-low-budget via 3-fold CV; best setting applied across tasks, sensitivity curves reported.  \n\nRobustness within experiment: not explicit – handled in exp-robustness.  \n\nEfficiency Measurement:\n• Use nvprof to capture FLOPs and kernel time per BO iteration (average of middle 50 % iterations)  \n• Measure peak GPU memory with torch.cuda.max_memory_allocated()  \n• Wall-clock recorded via Python’s time module, excluding plotting.  \n\nSelection Criterion for plots:\n• Best-validation log-likelihood model per seed (early stopping patience=30 iterations)  \n\nExample Code Snippet:\n```\nfrom mfbo import make_problem\nfrom radnn import RADNNMFBO, DNNSurrogate\nfrom utils import bo_loop\n\ntask = make_problem(\"branin\", n_fidelities=2)\nmodel = RADNNMFBO(task.x_dim) if variant==\"ra-dnn-mfbo\" else DNNSurrogate(...)\nresults = bo_loop(model, task, budget=150, seed=seed)\n```\nOutcome Expected: ≥10 % lower final regret and AUC on ≥70 % of problems, ≥15 % lower nRMSE, ≤2 % run-time overhead.\n"
        },
        {
            "experiment_id": "exp-robustness-generalisation",
            "run_variations": [
                "baseline-dnn-mfbo",
                "ra-dnn-mfbo",
                "mf-gp-ei",
                "ra-dnn-per-dim-gates"
            ],
            "description": "Objective / Hypothesis:\nTest H3 (robustness), H5 (generalisation) and H6 (interpretability).  RA-DNN-MFBO should gracefully degrade when fidelity correlations weaken, resist noise, and learn gate values that track correlation.\n\nStress Protocols:\nA. Correlation-Sweep Synthetic: continuously morph low-fidelity Branin variant f1(x;λ)=f0(x)+λ·sin(4πx₀) with λ∈[0,1].  \nB. Noise Injection: add i.i.d. N(0,σ²) noise to f1(x) with σ ∈ {0,0.1,0.2,0.5}.  \nC. Mid-Run Fidelity Shift: after 50 % budget, swap low-fidelity function to unrelated polynomial; measure recovery (regret slope).  \nD. Domain Transfer: hyper-parameter tuning of ResNet-20 on CIFAR-10 vs CIFAR-100 where low-fidelity = 10 epochs, high-fidelity = 160 epochs; evaluate on both datasets.\n\nPre-processing:\nSame normalisation as exp-main-perf. Images for CIFAR are standardised (RGB mean/std) and randomly-cropped then cut-out for training; no augmentation for validation/test.\n\nData Splitting:\n• Synthetic: 100 LH points hold-out per λ,σ for nRMSE.  \n• CIFAR: 45 K train / 5 K val / 10 K test. BO only sees train/val; report test accuracy of best-predicted configuration.\n\nRepetitions:\n5 seeds per λ or σ setting ⇒ 5×11 (λ grid) + 5×4 (σ) = 75 synthetic runs; 5 seeds × 2 domains for CIFAR.  \n\nEvaluation Metrics:\n• Robustness area: AUC of regret across λ or σ grid  \n• Recovery time (budget steps to beat baseline) after shift  \n• Test accuracy (CIFAR)  \n• Pearson r between learnt α and empirical corr(f0,f1)  \n• Overhead metrics identical to exp-main-perf.\n\nRun Variations (4):\n1. baseline-dnn-mfbo  \n2. ra-dnn-mfbo  \n3. mf-gp-ei – SOTA GP surrogate with auto-relevance determination  \n4. ra-dnn-per-dim-gates – ablation using per-dimension gates α_j (×input dim) to probe complexity/interpretability trade-off.\n\nHyper-parameter Sensitivity:\nFor RA variants sweep β (KL-weight) and gate initialisation a₀∈{0,1,−1} (sigmoid space) on λ=0.5 task; report heat-maps of robustness AUC.\n\nRobustness Analysis Methods:\n• Fit piece-wise linear model of regret vs λ; compute slope difference between methods.  \n• For noise experiments compute d(regret)/dσ.  \n• Statistical tests: paired Wilcoxon across seeds for each λ,σ.\n\nCompute & Efficiency:\nSame profiling as exp-main-perf; additionally log GPU util% and energy via nvidia-smi.  \n\nSelection Criterion:\nLast-epoch model (no early stop) to capture true robustness.\n\nExample Experimental Skeleton:\n```\nfor variant in VARIANTS:\n    for lam in np.linspace(0,1,11):\n        task = make_corr_sweep_branin(lam)\n        run_bo(task, variant, seeds=5)\n```\nExpected Outcome:\n• RA-DNN maintains ≤10 % regret increase up to λ=1, while baseline exceeds 25 %.  \n• After fidelity shift RA recovers within 10 steps vs 25 for baseline.  \n• Pearson r(α,corr) ≈0.8.  \n• Overhead <5 %.\n"
        }
    ],
    "expected_models": [
        "RADNNMFBO",
        "DNN-MFBO",
        "MF-GP-EI",
        "ResNet-20"
    ],
    "expected_datasets": [
        "Branin-2F",
        "Levy-2F",
        "BBOB-MF",
        "NAS-Bench-201-MF",
        "CIFAR-10",
        "CIFAR-100"
    ]
}
