
Input:
You are an expert reviewer for a top-tier international conference.
Please conduct a comprehensive review of the research paper provided, evaluating it according to the standards of venues like NeurIPS, ICML, ICLR, or AAAI.

Your task is to evaluate the paper on four key dimensions and provide scores from 1-10 for each:

## Evaluation Dimensions:

### 1. Novelty (1-10)
- How original and innovative is the proposed approach?
- Does it introduce new concepts, methods, or insights?
- Is there sufficient differentiation from existing work?

### 2. Significance (1-10)
- What is the potential impact of this work on the field?
- Does it address an important problem?
- Are the contributions meaningful and substantial?

### 3. Reproducibility (1-10)
- Are the experimental details sufficient for reproduction?
- Is the methodology clearly described?
- Are datasets, hyperparameters, and implementation details provided?

### 4. Experimental Quality (1-10)
- Are the experiments well-designed and comprehensive?
- Are appropriate baselines and evaluation metrics used?
- Is statistical significance properly assessed?
- Are the results convincing and well-analyzed?

## Section-by-Section Analysis:

For each section of the paper, provide:
- Key strengths
- Areas for improvement
- Specific comments on quality and completeness

## Overall Assessment:

Provide your scores for each dimension, followed by an overall recommendation.

## Paper Content:


**Title:** Reliability-Aware Gating for Deep Multi-Fidelity Bayesian Optimization


**Abstract:** Multi-fidelity Bayesian optimization accelerates black-box search by mixing cheap, biased evaluations with costly, accurate ones. Deep Neural Network MFBO (DNN-MFBO) models the high fidelity auto-regressively by concatenating the previous-fidelity prediction as an unscaled feature, an assumption that triggers negative transfer whenever cross-fidelity correlation is weak, non-stationary, or input-dependent [li-2020-multi]. We introduce Reliability-Aware DNN-MFBO (RA-DNN-MFBO), a minimal yet principled extension that inserts one learnable scalar gate per fidelity. Each gate Î±mâˆˆ[0,1] multiplicatively scales the lower-fidelity signal, is endowed with a Beta(1,1) prior, and is inferred variationally via a Gaussian posterior on its logit, adding only one KL term to the ELBO. The change preserves the original architecture, training loop, and mutual-information acquisition because scaling a Gaussian keeps it Gaussian. On two-fidelity Branin and Levy benchmarks, a 12-function BBOB-MF suite, and NAS-Bench-201-MF, RA-DNN-MFBO lowers cost-weighted simple regret by 10â€“25 %, reduces surrogate nRMSE by 15â€“20 %, and incurs <2 % runtime and memory overhead. Learned gates align with empirical inter-fidelity correlations, confirming adaptive behaviour. Comparisons with the original DNN-MFBO and a GP-based MFBO baseline demonstrate superior efficiency, robustness, and interpretability [li-2020-multi, li-2021-batch].


**Introduction:** Bayesian optimization (BO) provides an efficient framework for optimizing expensive, derivative-free objectives. Many real-world problems admit cheaper, lower-fidelity approximations of the target functionâ€”coarser simulations, reduced data, fewer training epochsâ€”whose evaluations cost less but are biased. Leveraging such hierarchies can slash the total budget, yet requires models and acquisition functions that correctly account for bias and varying noise.

Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) stacks Bayesian neural networks so that the m-th fidelity is conditioned on the prediction of fidelity mâˆ’1, and derives a tractable mutual-information acquisition via sequential Gaussâ€“Hermite quadrature and moment matching [li-2020-multi]. While powerful, the method hard-wires the previous-fidelity mean as an unscaled feature. This implicitly assumes that correlation is globally strong and stationary. In practice correlations may fade or even reverse across the input space, causing negative transfer: the surrogate over-trusts misleading low-fidelity outputs, its predictions deteriorate, and the acquisition wastes budget on suboptimal, low-cost queries.

We ask: can we preserve the efficiency and tractability of DNN-MFBO while letting the model learn how much to trust each lower fidelity? We answer in the affirmative with Reliability-Aware DNN-MFBO (RA-DNN-MFBO), a surgically small modification that introduces a single learnable gate per fidelity. The gate scales the lower-fidelity prediction before concatenation, has a uniform Beta prior, and is inferred jointly with network weights through variational Bayes. If correlation is weak, the posterior drives the gate toward zero, muting harmful influence; if correlation is strong, the gate approaches one, recovering the original architecture. Because multiplication by a scalar preserves Gaussianity, the acquisition derivations and computational profile remain untouched.

The challenge lies in balancing adaptability, tractability, and cost. Many alternative fixesâ€”input-dependent kernels, attention mechanisms, or fidelity-specific hyper-networksâ€”break the mathematical structure that renders DNN-MFBO scalable, or introduce prohibitive overhead. Our approach meets four design goals: (1) adaptability to varying reliability; (2) full compatibility with existing acquisition machinery; (3) negligible parameter and runtime cost; (4) interpretable diagnostics via the learnt gate values.

We validate RA-DNN-MFBO on synthetic two-fidelity Branin and Levy functions, the 12-function BBOB-MF suite, and the neural architecture search benchmark NAS-Bench-201-MF. Across ten seeds we report cost-weighted simple regret, surrogate accuracy (nRMSE), runtime, memory, robustness to correlation shifts and noise, and correlation between learnt gates and empirical reliability. Results consistently favour RA-DNN-MFBO over the original DNN-MFBO and a strong GP-based MFBO baseline. We further position our contribution relative to batch deep MFBO with auto-regressive networks (DARN) [li-2021-batch], emphasising that our gating is orthogonal and can be combined with such acquisition extensions.

Contributions:
â€¢ A reliability-aware scalar gate for deep multi-fidelity surrogates requiring only one extra parameter and a small KL term.
â€¢ Proof of compatibility with DNN-MFBOâ€™s moment-matching acquisition, preserving tractability.
â€¢ Extensive empirical study showing 10â€“25 % lower simple regret, â‰ˆ18 % better nRMSE, and <2 % overhead across diverse tasks.
â€¢ Robustness analyses demonstrating immunity to negative transfer and interpretable gate dynamics that track fidelity correlation.

Looking ahead, richer gatesâ€”per-dimension or input-dependentâ€”may capture local reliability at modest additional cost, and joint adaptation of acquisition and gating could yield further gains. Nonetheless, this work shows that carefully chosen minimal changes can deliver outsized benefits for multi-fidelity BO.


**Related Work:** Gaussian-process (GP)-based multi-fidelity BO dominates early literature, modelling correlations via co-kriging or hierarchical kernels. These models provide principled uncertainty but scale poorly with input dimension and struggle when cross-fidelity mappings are highly nonlinear. Neural surrogates address these issues. DNN-MFBO introduced an auto-regressive Bayesian neural network and a tractable mutual-information acquisition, outperforming GP counterparts on several benchmarks [li-2020-multi]. Our work retains their surrogate but removes the rigid assumption of perfect trust by inserting a learnable gate.

Batch deep MFBO with Deep Auto-Regressive Networks (DARN) extends DNN-MFBO to batch querying by designing a Max-value Entropy Search acquisition that penalises redundant queries [li-2021-batch]. DARN focuses on acquisition-side improvements; our contribution focuses on surrogate reliability. The two are complementary: gating can be applied within DARN unchanged.

Alternative reliability schemes exist. Some GP methods learn input-dependent noise or apply warped kernels, increasing inference cost. Neural approaches have explored attention-based fusion of fidelities, but these layers complicate moment matching, breaking analytical acquisition computation. Our gate keeps the mathematical form unchanged and adds virtually no computation.

Finally, modern ML systems like TensorFlow enable large-scale experiments [abadi-2016-system], yet system advances alone cannot prevent negative transfer. RA-DNN-MFBO tackles the modelling root cause, providing a lightweight, interpretable fix without requiring system-level tweaks.


**Background:** Problem setting. Let XâŠ‚â„d be the design space and {f1,â€¦,fM} a hierarchy of fidelities with evaluation costs 1â‰ªc2<â€¦<cM. Querying (x,m) yields ym(x)=fm(x)+Ïµm, Ïµmâˆ¼ð’©(0,Ïƒm2). The objective is to minimise fM within a total cost budget. DNN-MFBO represents each fidelity m as a Bayesian neural network receiving the design x concatenated with the predictive mean of fidelity mâˆ’1, thereby forming an auto-regressive chain. Predictive distributions are Gaussian, enabling moment matching. The mutual-information acquisition selects the next query by maximising expected information gain per unit cost and is evaluated via sequential Gaussâ€“Hermite quadrature [li-2020-multi].

Negative transfer. When Cov(fmâˆ’1,fm) is small or sign-flips locally, the unscaled inclusion of fÌ‚mâˆ’1 biases the surrogate toward false trends, inflating predictive error and misguiding the acquisition. The phenomenon is exacerbated when low-fidelity queries are far cheaper, tempting the acquisition to oversample them.

Gated coupling. We introduce Î±mâˆˆ[0,1] such that the m-th network input becomes [x;Î±m fÌ‚mâˆ’1(x)]. The gate moderates information flow. A uniform Beta(1,1) prior encodes initial ignorance. Re-parametrising with zm=logit(Î±m) and placing a Gaussian variational posterior q(zm) allows closed-form KL to the Beta prior, adding one term to the ELBO. Because scaling a Gaussian by a constant preserves its family, all predictive moments required for acquisition remain analytically available. Thus the entire DNN-MFBO pipeline remains intact while gaining adaptability.


**Method:** For fidelities m=2,â€¦,M, let fmÎ¸m denote the Bayesian neural network with weights Î¸m. Given design x, the predictive mean of fidelity mâˆ’1 is Âµmâˆ’1(x). RA-DNN-MFBO forms the input vector xm=[x;Î±m Âµmâˆ’1(x)] and outputs a Gaussian N(Âµm(x),Ïƒm2(x)). The gate Î±m=Ïƒ(sigmoid)(wm) with unconstrained parameter wm. Priors: p(Î¸m) standard Gaussian as in DNN-MFBO; p(Î±m)=Beta(1,1). Variational posteriors: q(Î¸m) factorised Gaussians; q(wm)=ð’©(Âµw,Ïƒw2). The ELBO is
ELBO=âˆ‘m E_q[log p(ym|x,Î¸m,Î±m)] âˆ’ âˆ‘m KL(q(Î¸m)||p(Î¸m)) âˆ’ âˆ‘m>1 KL(q(Î±m)||p(Î±m)).
The only change w.r.t. DNN-MFBO is the last term; implementation requires two code lines. Gradient-based optimisation proceeds with the reparameterisation trick.

Acquisition computation. The mutual-information criterion relies on predictive means and variances of each fidelity. Scaling Âµmâˆ’1 by Î±m simply multiplies its mean and variance by Î±m and Î±m2, respectively; Gaussâ€“Hermite quadrature and moment matching stay valid. Therefore, acquisition values, their gradients, and fidelity-selection mechanisms are reused verbatim.

Complexity. The model adds one scalar parameter and two scalar moments per fidelity. Computational overhead is O(1) per forward pass and imperceptible in practice (<2 % wall-clock). The gate can just as easily be inserted into batch variants such as DARN without any further derivation.


**Experimental Setup:** Benchmarks. (1) Two-fidelity Branin and Levy functions supplied with DNN-MFBO. Low fidelity is a cheap interpolation; high fidelity is exact. (2) BBOB-MF suite: 12 analytic functions with designed fidelity gaps. (3) NAS-Bench-201-MF, where low fidelity is a network trained for 12 epochs and high fidelity for 200 epochs.

Protocol. Each method starts from the same 10 random queries per fidelity. Budgets: 150 cost units on synthetic tasks (costs 1 and 10), equivalent cost ratios on BBOB-MF, and 8 GPU-hours on NAS. Ten random seeds for performance studies, five for robustness sweeps.

Metrics. Primary: cost-weighted simple regret curve and its area under curve (AUC). Modelling: nRMSE of the high-fidelity surrogate on 100 held-out points (synthetic) or the validation set (NAS). Efficiency: wall-clock per BO step, peak GPU memory. Robustness: slope of regret versus correlation or noise, steps to recovery after fidelity shift. Interpretability: Pearson correlation between learnt Î± and empirical inter-fidelity correlation.

Baselines. Original DNN-MFBO [li-2020-multi]; multi-fidelity GP with expected improvement; ablations fixed Î±=1, fixed Î±=0, no-KL, and per-dimension gates. All share architecture and optimiser settings; only the presence or behaviour of Î± differs.

Implementation. PyTorch; deterministic cuDNN; single A100-80 GB GPU; mixed precision off. The training loop mirrors that in the DNN-MFBO repository with two added lines for the KL on Î±. Logs are captured with Weights-and-Biases. Statistical significance uses paired t-tests with Holm correction.


**Results:** Main performance. On Branin, RA-DNN-MFBO attains final simple regret 0.021 Â± 0.003 versus 0.028 Â± 0.004 for DNN-MFBO, a 24.8 % improvement (p=4Â·10â»Â³). Surrogate nRMSE drops from 0.082 Â± 0.006 to 0.067 Â± 0.005. The gate converges to Î±=0.91 Â± 0.02, reflecting strong correlation. On Levy, regret falls by 15.7 % and nRMSE by 18.8 %, with Î±=0.32 Â± 0.04, demonstrating attenuation under weak correlation.

Across BBOB-MF, median regret improves by 11.4 %; 10 of 12 functions show statistically significant gains. Mean nRMSE improves by 17.6 %. In NAS-Bench-201-MF, validation simple regret halves (0.007 vs 0.013) and the 92 % accuracy threshold is reached in 2.8 GPU-h versus 5.2 GPU-h.

Ablations. Fixed Î±=1 reproduces the baseline. Fixed Î±=0 harms performance by 32 %, confirming that low-fidelity data are useful when properly gated. Removing the KL causes occasional gate saturation and intermediate performance. Per-dimension gates cut nRMSE by a further 2 % but cost 6 % more parameters and 4.8 % runtime.

Efficiency. The additional gate parameter increases model size by 0.03 %. Wall-clock per BO step rises by 1.7 Â± 0.4 %, GPU memory by 1.2 %, both well under the 5 % target.

Robustness. When low-fidelity correlation is swept from strong to adversarial, the regret slope is 0.024 (RA) versus 0.058 (baseline); at zero correlation, RAâ€™s regret is 0.047 Â± 0.006 while the baselineâ€™s is 0.081 Â± 0.008 (42 % worse). Gate values correlate with empirical correlation (r=0.83, p<10â»âµ). Under injected noise Ïƒ=0.5, RA outperforms by 38 %. After a mid-run fidelity swap, RA recovers in 7 Â± 2 steps whereas the baseline needs 22 Â± 4.

Comparison to GP-based MFBO. RA beats the GP baseline by 9 % (Branin), 13 % (Levy) and 7 % median on BBOB-MF, while being 3â€“4Ã— faster and using less memory on NAS.

Limitations. Scalar gating cannot model input-dependent reliability; richer gates may improve further. Acquisition design remains as in DNN-MFBOâ€”jointly learning acquisition parameters and gates is future work.


**Conclusion:** Reliability-Aware DNN-MFBO augments deep multi-fidelity surrogates with a single learnable gate per fidelity, governed by a simple Beta prior and inferred through variational Bayes. The gate adaptively scales lower-fidelity information, preventing negative transfer when correlations are weak and seamlessly recovering the original model when correlations are strong. Crucially, the modification leaves DNN-MFBOâ€™s analytical acquisition untouched and adds negligible computational cost.

Experiments across synthetic functions, black-box optimisation suites, and neural architecture search show 10â€“25 % lower cost-weighted simple regret, â‰ˆ18 % better surrogate accuracy, and <2 % overhead. Robustness tests confirm improved resilience to correlation shifts, noise, and fidelity swaps, with gate values providing an interpretable reliability signal. Comparisons against GP-based and ablated models corroborate the benefits.

The study demonstrates that minimal, principled adjustments to surrogate coupling can yield significant gains. Future directions include input-dependent or per-dimension gating, joint optimisation of acquisition hyper-parameters with gate learning, and application to large-scale scientific design problems. The proposed gate is a drop-in upgrade for any stacked neural surrogate, offering an immediate path to more reliable and efficient multi-fidelity Bayesian optimisation.


Output:
{
    "novelty_score": 6,
    "significance_score": 7,
    "reproducibility_score": 8,
    "experimental_quality_score": 8
}
