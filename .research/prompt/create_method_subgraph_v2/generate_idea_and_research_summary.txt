
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    Improving efficiency of hyperparameter optimization
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks",
    "Main Contributions": "The paper proposes Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) to address the inefficiency of existing multi-fidelity BO methods that oversimplify or ignore complex correlations across fidelities. DNN-MFBO uses a deep neural network-based surrogate model to flexibly capture highly nonlinear and nonstationary relationships between fidelities, thereby improving objective function estimation. It also introduces a computationally tractable and efficient mutual information-based acquisition function, calculated using sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching, which enables maximum entropy search. The method demonstrates superior optimization performance and cost-efficiency on synthetic benchmarks and real-world engineering design problems.",
    "Methodology": "DNN-MFBO employs a stacked architecture of neural networks, where each NN models one fidelity. For fidelities m > 1, the NN's input concatenates the original input with the output from the previous fidelity, `xm = [x; fm-1(x)]`, allowing information propagation and complex relationship learning. The NN output is `fm(x) = w^T_m φ_θ_m (xm)`. Output layer weights `wm` are treated as random variables with a standard normal prior, while other NN weights `θm` are hyper-parameters. A stochastic variational learning algorithm, leveraging the reparameterization trick, jointly estimates the Gaussian posterior `q(wm)` and hyper-parameters by maximizing a variational evidence lower bound (ELBO). The acquisition function is `a(x, m) = (1/λm) * I(f*, fm(x)|D)`. Computing this involves approximating posterior output distributions `p(fm(x)|D)` as Gaussian using fidelity-wise moment matching and Gauss-Hermite quadrature. The conditional entropy term `Ep(f*|D)[H(fm(x)|f*, D)]` is approximated via Monte-Carlo sampling of `f*` and by computing `H(fm(x)|fM(x) ≤ f*, D)`, again using sequential quadrature and moment matching to approximate conditional posteriors as Gaussian. Acquisition function optimization uses automatic differentiation and L-BFGS.",
    "Experimental Setup": "DNN-MFBO was evaluated on three synthetic benchmark functions: Branin (3 fidelities, 2D input), Park1 (2 fidelities, 4D input), and Levy (3 fidelities, 2D input). It was also tested on two real-world engineering design applications: Mechanical Plate Vibration Design (3 material properties, 2 fidelities, maximizing frequency) and Thermal Conductor Design (3 shape parameters, 2 fidelities, minimizing heat conduction time). Competing methods included Multi-Fidelity Sequential Kriging (MF-SKO), MF-GP-UCB, Multi-Fidelity Predictive Entropy Search (MF-PES), Multi-Fidelity Maximum Entropy Search (MF-MES), Multi-task NN based BO (MTNN-BO), and Single Fidelity MES (SF-MES). Initial training points were randomly queried across fidelities (e.g., 20, 20, 2 samples for Branin/Levy's 3 fidelities; 20, 5 samples for real-world 2 fidelities). Query costs were defined as (λ1, λ2, λ3) = (1, 10, 100) or (λ1, λ2) = (1, 10). Performance was measured by simple regret, inference regret, queried function values, and average query time. Experiments were repeated five times, reporting average results and standard error bars. DNN-MFBO and MTNN-BO were implemented with TensorFlow, using ReLU activation and ADAM optimizer. Hyper-parameters were tuned using SMAC3 and manual tuning.",
    "Limitations": "The calculation of the conditional entropy `H(fm(x)|f*, D)` within the acquisition function is analytically intractable, leading to an approximation using `H(fm(x)|fM(x) ≤ f*, D)`. Moment matching is repeatedly used to approximate non-Gaussian distributions as Gaussian, which inherently introduces approximations. The ELBO for variational learning is analytically intractable, requiring stochastic optimization. The current work focuses exclusively on discrete fidelities, not addressing continuous fidelity settings. Furthermore, NN architecture and learning rates require tuning, which was done via an AutoML tool (SMAC3) followed by manual adjustments, indicating a potential need for careful configuration.",
    "Future Research Directions": "Not mentioned explicitly as future work, but the paper states a focus on discrete fidelities, implying that extending the DNN-MFBO framework to handle continuous fidelities could be a potential area for future research. This would involve adapting the fidelity modeling and acquisition function calculations for continuous fidelity spaces.",
    "Experiment Code": "class b2WorldInterface:\n    def __init__(self, do_gui=True):\n        self.world = b2World(gravity=(0.0,0.0), doSleep=True)\n        self.do_gui = do_gui\n        self.TARGET_FPS = 100\n        self.TIME_STEP = 1.0/self.TARGET_FPS\n        self.VEL_ITERS, self.POS_ITERS =10,10\n        self.bodies = []\n\n        if do_gui:\n            self.gui_world  = guiWorld(self.TARGET_FPS)\n            #raw_input()\n        else:\n            self.gui_world = None\n\n    def initialize_gui(self):\n        if self.gui_world == None:\n            self.gui_world = guiWorld(self.TARGET_FPS)\n        self.do_gui = True\n    def stop_gui(self):\n        self.do_gui = False\n\n    def add_bodies(self, new_bodies):\n        \"\"\" add a single b2Body or list of b2Bodies to the world\"\"\"\n        if type(new_bodies) == list:\n            self.bodies += new_bodies\n        else:\n            self.bodies.append(new_bodies)\n    def step(self, show_display=True, idx=0):\n        self.world.Step(self.TIME_STEP, self.VEL_ITERS, self.POS_ITERS)\n        if show_display and self.do_gui:\n            self.gui_world.draw(self.bodies)\n            #if idx % 10 == 0:\n            #    pygame.image.save(self.gui_world.screen,'tmp_images/'+str(int(sm.ttt*100)+idx)+'.bmp')\n\nclass end_effector:\n    def __init__(self, b2world_interface, init_pos, base, init_angle, hand_shape='rectangle', hand_size=(0.3,1)):\n        world= b2world_interface.world\n        self.hand = world.CreateDynamicBody(position=init_pos,angle=init_angle)\n        self.hand_shape = hand_shape\n        self.hand_size = hand_size\n        # forceunit for circle and rect\n        if hand_shape == 'rectangle':\n            rshape = b2PolygonShape(box=hand_size)\n            self.forceunit = 30.0\n        elif hand_shape == 'circle':\n            rshape = b2CircleShape(radius=hand_size)\n            self.forceunit = 100.0\n        elif hand_shape == 'polygon':\n            rshape = b2PolygonShape(vertices=hand_size)\n        else:\n            raise Exception(\"%s is not a correct shape\" % hand_shape)\n\n        self.hand.CreateFixture(\n            shape = rshape,\n            density = .1,\n            friction = .1\n            )\n        self.hand.userData = \"hand\"\n        \n        friction_joint = world.CreateFrictionJoint(\n            bodyA = base,\n            bodyB = self.hand,\n            maxForce = 2,\n            maxTorque = 2,\n            )\n        b2world_interface.add_bodies(self.hand)\n\n        \n\n    def set_pos(self, pos, angle):\n        self.hand.position = pos\n        self.hand.angle = angle\n    def apply_wrench(self, rlvel=(0,0), ravel=0):\n        #self.hand.ApplyForce(force, self.hand.position,wake=True)\n        #if avel != 0:\n        \n        avel = self.hand.angularVelocity\n        delta_avel = ravel - avel\n        torque = self.hand.mass*delta_avel*30.0\n        self.hand.ApplyTorque(torque, wake=True)\n        \n        #else:\n        lvel = self.hand.linearVelocity\n        delta_lvel = b2Vec2(rlvel) - b2Vec2(lvel)\n        force = self.hand.mass*delta_lvel*self.forceunit\n        self.hand.ApplyForce(force, self.hand.position,wake=True)\n        \n\n    def get_state(self, verbose=False):\n        state = list(self.hand.position) + [ self.hand.angle] +  \\\n                list(self.hand.linearVelocity) + [self.hand.angularVelocity]\n        if verbose:\n            print_state = [\"%.3f\" % x for x in state]\n            print \"position, velocity: (%s), (%s) \" % \\\n                ((\", \").join(print_state[:3]), (\", \").join(print_state[3:]) )\n    \n        return state\n\ndef make_1thing(base, b2world_interface, thing_shape, thing_size, thing_friction, thing_density, obj_loc):\n    world = b2world_interface.world\n    \n    link = world.CreateDynamicBody(position=obj_loc)\n    if thing_shape == 'rectangle':\n        linkshape = b2PolygonShape(box=thing_size)\n    elif thing_shape == 'circle':\n        linkshape = b2CircleShape(radius=thing_size)\n    elif thing_shape == 'polygon':\n        linkshape = b2PolygonShape(vertices=thing_size)\n    else:\n        raise Exception(\"%s is not a correct shape\" % thing_shape)\n    \n    link.CreateFixture(\n            shape = linkshape, \n            density = thing_density,\n            friction = thing_friction,\n            )\n    friction_joint = world.CreateFrictionJoint(\n            bodyA = base,\n            bodyB = link,\n            maxForce = 5,\n            maxTorque = 2,\n            )\n\n    b2world_interface.add_bodies([link])\n    return link\ndef simu_push_2robot2thing(world, thing, thing2, robot, robot2, base, xvel, yvel, xvel2, yvel2, rtor, rtor2, simulation_steps, simulation_steps2):\n    desired_vel = np.array([xvel, yvel])\n    rvel = b2Vec2(desired_vel[0]+np.random.normal(0,0.01),desired_vel[1]+np.random.normal(0,0.01))\n    \n    desired_vel2 = np.array([xvel2, yvel2])\n    rvel2 = b2Vec2(desired_vel2[0]+np.random.normal(0,0.01),desired_vel2[1]+np.random.normal(0,0.01))\n    tmax = np.max([simulation_steps,simulation_steps2])\n    for t in range(tmax+100):\n        if t < simulation_steps:\n            robot.apply_wrench(rvel, rtor)\n        if t < simulation_steps2:\n            robot2.apply_wrench(rvel2, rtor2)\n        world.step()\n\n    return (list(thing.position), list(thing2.position))\ndef make_base(table_width, table_length, b2world_interface):\n    world = b2world_interface.world\n    base = world.CreateStaticBody(\n            position = (0,0),\n            #friction = base_friction,\n            shapes = b2PolygonShape(box=(table_length,table_width)),\n            )\n   \n\n    b2world_interface.add_bodies([base])\n    return base",
    "Experiment Result": "The provided repository content does not contain the implementation of the DNN-MFBO method itself. Instead, it provides a physics-based simulation environment that the DNN-MFBO method would likely be applied to or optimize. The extracted code sections define the core components and logic of this simulation environment.\n\n**Experimental Setup for DNN-MFBO:**\n- **Task:** The objective is to optimize control parameters for one or two robots to push one or two objects (circular or rectangular) towards specific target goal positions in a 2D Box2D physics simulation.\n- **Design Variables (Input `x` for DNN-MFBO):** The parameters that DNN-MFBO would optimize include:\n    - Initial robot positions (`rx`, `ry`).\n    - Robot push velocities (`xvel`, `yvel` for each robot).\n    - Robot initial angles or applied torques (`init_angle`, `rtor` for each robot).\n    These parameters directly influence the push actions within the simulation.\n- **Objective Function (High Fidelity `fM(x)` for DNN-MFBO):** The goal is to minimize the total Euclidean distance between the final position(s) of the pushed object(s) and their respective target goal position(s). For multiple objects, this is the sum of individual distances (e.g., `np.linalg.norm(goal1 - final_pos1) + np.linalg.norm(goal2 - final_pos2)`).\n- **Fidelity Parameter (`m` for DNN-MFBO):** The `simulation_steps` parameter (e.g., `simu_steps`, `simu_steps2`) directly controls the duration of the physics simulation. This serves as the fidelity variable (`m`) for DNN-MFBO. A lower number of `simulation_steps` corresponds to a lower-fidelity (faster, less computationally expensive, but potentially less accurate) simulation, while a higher number corresponds to a higher-fidelity simulation.\n- **Simulation Details:**\n    - Uses the Box2D physics engine (`pybox2d`).\n    - Simulates object-robot interactions, including applying forces and torques.\n    - Adds slight Gaussian noise (`np.random.normal`) to desired velocities during robot control. The simulation time step is `1.0/100` seconds."
}{
    "Title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks",
    "Main Contributions": "This research proposes Batch Multi-fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN) to address the optimization of expensive, black-box functions with multi-fidelity evaluations. Its main contributions are: (1) A deep auto-regressive model utilizing Bayesian neural networks capable of capturing complex, strong, nonstationary, and nonlinear relationships across all fidelities, thereby improving surrogate learning. (2) A simple yet efficient batch querying method based on the Max-value Entropy Search (MES) principle, designed to explicitly penalize highly correlated queries and promote diversity within the batch. (3) An alternating optimization algorithm for maximizing the batch acquisition function, which efficiently handles mixed continuous inputs and discrete fidelities without combinatorial search. (4) Empirical validation demonstrating superior performance on both synthetic benchmarks and four real-world hyperparameter optimization tasks (CNN, Online LDA, XGBoost, Physics-Informed Neural Networks) in terms of finding better hyperparameters with smaller total evaluation costs.",
    "Methodology": "BMBO-DARN employs a deep auto-regressive model as its surrogate. This model consists of a chain of M Bayesian neural networks (NNs), where each NN `fm(x)` models a specific fidelity. The input `xm` for `fm(x)` includes not only the original input `x` but also the outputs of all previous fidelities `f1(x), ..., fm-1(x)`. This allows the model to capture complex inter-fidelity relationships. Hamiltonian Monte-Carlo (HMC) sampling is used for robust posterior inference of the NN weights and noise precisions. For batch querying, a batch acquisition function is proposed, based on the Max-value Entropy Search (MES) principle, which measures the mutual information between the queried batch outputs and the function optimum, normalized by the total query cost. This function explicitly penalizes highly correlated queries to ensure diversity. To compute this acquisition function efficiently, posterior samples of NN weights are used with moment matching to approximate the joint posterior of the batch outputs and function optimum as a multivariate Gaussian distribution, allowing for a closed-form calculation of mutual information. To maximize the acquisition function over mixed continuous inputs and discrete fidelities, an alternating optimization algorithm is developed. This algorithm cyclically updates one input-fidelity pair at a time while fixing others, avoiding an exponentially costly combinatorial search and guaranteeing improvement at each step.",
    "Experimental Setup": "The method's performance was evaluated in two main categories: surrogate learning and real-world hyperparameter optimization. For surrogate learning, two synthetic benchmark functions were used: Levy (two fidelities) and Branin (three fidelities), both featuring nonlinear/nonstationary transformations between fidelities. Performance was measured using Normalized Root-Mean-Square-Error (nRMSE) and Mean-Negative-Log-Likelihood (MNLL) on 100 uniformly sampled test points. For real-world applications, BMBO-DARN was applied to hyperparameter optimization for four machine learning models: Convolutional Neural Networks (CNN) on CIFAR-10, Online Latent Dirichlet Allocation (LDA) on 20NewsGroups, XGBoost for diabetes diagnosis, and Physics-Informed Neural Networks (PINN) for solving Burger's equation. Each application involved 3 fidelities (e.g., training epochs, number of weak learners, L-BFGS iterations) with defined cost ratios. Metrics included negative log-loss for CNN, perplexity for LDA, and log nRMSE for XGBoost and PINN. Comparative methods included state-of-the-art multi-fidelity BO algorithms (MF-GP-UCB, MF-MES, SHTL, DNN-MFBO, MF-MES-Batch), single-fidelity BO (SF-Batch), and other popular hyperparameter tuning methods (SMAC3, Hyperband, BOHB), along with an ablation (BMBO-DARN-1, batch size 1). All batch querying methods used a batch size of 5. Experiments were initialized with 10 random queries at each fidelity and repeated 5 times, reporting average performance against accumulated cost (running time). The implementation used PyTorch, with HMC sampling via Hamiltorch (5K burn-in steps, 200 posterior samples, 10 leapfrog steps, 0.012 step size).",
    "Limitations": "The paper does not explicitly detail limitations of the proposed BMBO-DARN method. However, based on the methodology, potential implicit limitations include: (1) The alternating optimization algorithm, while efficient and guaranteed to improve at each step, does not explicitly guarantee finding the global optimum of the batch acquisition function. (2) Hamiltonian Monte Carlo (HMC) for posterior inference, despite providing unbiased and high-quality uncertainty quantification, can be computationally intensive, although the authors mention the flexibility to switch to other approximate inference methods like SGHMC.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": null,
    "Experiment Result": null
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "DNN-MFBO hard-wires the previous-fidelity output fm-1(x) as an unscaled input feature for fm(x).\nIf the correlation between fidelities is weak or changes locally, this unconditional use of fm-1(x) can introduce negative transfer, slowing hyper-parameter search and occasionally selecting misleading low-cost queries. A minimal mechanism is needed to adaptively reduce or amplify the influence of lower fidelities without redesigning the architecture or acquisition.",
    "methods": "Reliability-Aware DNN-MFBO (RA-DNN-MFBO)\nMinimal change: insert one learnable scalar gate αm∈[0,1] for every fidelity m>1.\nInput becomes   xm=[x ; αm·fm-1(x)]   instead of   [x ; fm-1(x)].\nPrior: αm∼Beta(1,1)  (uniform). In variational inference we keep a Gaussian variational posterior on logit(αm). An extra KL( q(αm) || p(αm) ) term is added to the ELBO (two additional lines of code).\nMotivation:\n• If fm-1 is unreliable, the posterior pushes αm→0, avoiding negative transfer.\n• When fidelities are strongly correlated the network learns αm→1, recovering the original model.\n• This single multiplicative gate is differentiable, cheap, and does not change acquisition-function derivations (moment matching still applies because scaling a Gaussian keeps it Gaussian).",
    "experimental_setup": "Data: 2-fidelity Branin and Levy benchmarks supplied with DNN-MFBO code (low-fidelity = cheap interpolation, high-fidelity = ground truth).\nBaselines: Original DNN-MFBO vs proposed RA-DNN-MFBO.\nInitial design: 10 random queries per fidelity.\nBudget: cost-weighted budget of 150 (low-fidelity cost 1, high-fidelity cost 10).\nMetrics: simple regret w.r.t. cost, plus nRMSE of surrogate on 100 test points to isolate modelling effect.\nRepeats: 10 random seeds.",
    "experimental_code": "import torch, torch.nn as nn\n\nclass FidelityBlock(nn.Module):\n    def __init__(self, in_dim, hid=50):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(in_dim, hid), nn.ReLU(),\n                                 nn.Linear(hid, 1))\n        self.log_var = nn.Parameter(torch.zeros(1))  # homoscedastic noise\n    def forward(self, x):\n        mean = self.net(x)\n        var  = torch.exp(self.log_var)\n        return mean, var\n\nclass RADNNMFBO(nn.Module):\n    \"\"\"2-fidelity version for demo\"\"\"\n    def __init__(self, x_dim):\n        super().__init__()\n        self.f1 = FidelityBlock(x_dim)\n        self.alpha_raw = nn.Parameter(torch.zeros(1))  # sigmoid→α∈(0,1)\n        self.f2 = FidelityBlock(x_dim+1)\n    def forward_f1(self, x):\n        return self.f1(x)\n    def forward_f2(self, x):\n        with torch.no_grad():\n            mean1,_ = self.f1(x)\n        alpha = torch.sigmoid(self.alpha_raw)\n        inp = torch.cat([x, alpha*mean1], dim=-1)\n        return self.f2(inp)\n# --- tiny training loop for surrogate only ---\nopt = torch.optim.Adam(model.parameters(), 1e-3)\nfor itr in range(1000):\n    m1,v1 = model.forward_f1(x1);  m2,v2 = model.forward_f2(x2)\n    nll  = ((y1-m1)**2/v1 + torch.log(v1)).mean()\n    nll += ((y2-m2)**2/v2 + torch.log(v2)).mean()\n    # Beta(1,1) prior KL on α (Gaussian approx):\n    kl_alpha = -torch.distributions.Beta(1,1).log_prob(torch.sigmoid(model.alpha_raw))\n    loss = nll + 0.01*kl_alpha\n    opt.zero_grad(); loss.backward(); opt.step()",
    "expected_result": "Across both test functions RA-DNN-MFBO attains ~5-10% lower simple regret after the same cost budget and 15-20% lower nRMSE of the high-fidelity surrogate.\nGating coefficients converge towards α≈0.3 on Levy (weak correlation) and α≈0.9 on Branin (strong correlation), confirming adaptive behaviour. Runtime overhead is negligible (<2%).",
    "expected_conclusion": "Introducing a single learnable gate per fidelity is enough to prevent negative transfer when fidelities are weakly correlated, while leaving the original behaviour intact when correlations are strong. The change is architecturally trivial, requires only one extra parameter and a tiny KL term, yet yields consistent efficiency gains in hyper-parameter optimization. Such reliability-aware gating can be plugged into any stacked multi-fidelity neural surrogate with virtually no additional computational cost."
}
